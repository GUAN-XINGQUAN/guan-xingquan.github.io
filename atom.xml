<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Guan&#39;s Website</title>
  
  
  <link href="http://example.com/atom.xml" rel="self"/>
  
  <link href="http://example.com/"/>
  <updated>2021-03-06T22:04:08.032Z</updated>
  <id>http://example.com/</id>
  
  <author>
    <name>Xingquan Guan</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Hello World</title>
    <link href="http://example.com/2022/05/29/hello-world/"/>
    <id>http://example.com/2022/05/29/hello-world/</id>
    <published>2022-05-29T23:21:17.808Z</published>
    <updated>2021-03-06T22:04:08.032Z</updated>
    
    <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;Welcome to &lt;a href=&quot;https://hexo.io/&quot;&gt;Hexo&lt;/a&gt;! This is your very first post. Check &lt;a href=&quot;https://hexo.io/docs/&quot;&gt;documentation&lt;/a&gt; for</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title>News and Updates</title>
    <link href="http://example.com/2022/05/29/News-and-Updates/"/>
    <id>http://example.com/2022/05/29/News-and-Updates/</id>
    <published>2022-05-29T07:01:45.000Z</published>
    <updated>2022-11-01T00:17:51.437Z</updated>
    
    <content type="html"><![CDATA[<h4 id="News-and-Updates"><a href="#News-and-Updates" class="headerlink" title="News and Updates:"></a><font size=5><em><strong>News and Updates:</strong></em></font></h4><p>October 20, 2022: Together with <a href="https://samueli.ucla.edu/people/henry-burton/">Prof. Henry Burton</a>, I have published a reserach paper entitled <em>Bias-Variance Tradeoff in Machine Learning: Theoretical Formulation and Implications to Structural Engineering Applications</em>. This <a href="https://authors.elsevier.com/a/1fyJm8MoIGwz2w">link</a> provides a free access to the paper until December 10, 2022. This is a <a href="https://www.youtube.com/watch?v=qVwCSXN8O5A">3-minute video synopsis</a> to summarize the paper.</p><p>March 17, 2022: I started a new quest as a senior data scientist at <a href="https://www.zest.ai/">ZEST AI</a>. I will use my expertise to help decision making in finance world. </p><p>May 1, 2021: I started my new journey as a postdoctoral scholar at UCLA. I am working with Prof. Henry Burton and Prof. Yousef Bozorgnia.</p><p>March 16, 2021: My Ph.D. dissertation was approved by all doctoral committee members and the UCLA graduation division. I have met all the doctoral degree requirements. Officially I became a Ph.D.</p><p>March 3, 2021: I passed Ph.D. final defense exam at the University of California, Los Angeles. My dissertation title is “Performance-based analytics-driven seismic design of steel moment frame buildings”.</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h4 id=&quot;News-and-Updates&quot;&gt;&lt;a href=&quot;#News-and-Updates&quot; class=&quot;headerlink&quot; title=&quot;News and Updates:&quot;&gt;&lt;/a&gt;&lt;font size=5&gt;&lt;em&gt;&lt;strong&gt;News and Upd</summary>
      
    
    
    
    
    <category term="news" scheme="http://example.com/tags/news/"/>
    
  </entry>
  
  <entry>
    <title>Introduction</title>
    <link href="http://example.com/2022/05/28/Introduction/"/>
    <id>http://example.com/2022/05/28/Introduction/</id>
    <published>2022-05-29T05:10:51.000Z</published>
    <updated>2022-05-30T05:03:37.857Z</updated>
    
    <content type="html"><![CDATA[<h3 id="Brief-Introduction"><a href="#Brief-Introduction" class="headerlink" title="Brief Introduction"></a>Brief Introduction</h3><p>Hi there!</p><p>My name is Xingquan Guan and I’m a Senior Data Scientist at <a href="https://www.zest.ai/">ZEST AI</a>. I received my Ph.D. in Structural Engineering with a Minor in Statistics and Computer Science at the University of California, Los Angeles. My expertise is to apply artificial intelligence in solving engineering related problems. Particularly, my research interests include:</p><ol><li>Application of artificial intelligence in enhancing community resilience</li><li>Experimental test and finite element analysis of building structures under extreme loading</li><li>Adoption of novel devices to improve seismic performance of conventional structures</li><li>Development of advanced computational simulation platforms for design automation</li></ol>]]></content>
    
    
      
      
    <summary type="html">&lt;h3 id=&quot;Brief-Introduction&quot;&gt;&lt;a href=&quot;#Brief-Introduction&quot; class=&quot;headerlink&quot; title=&quot;Brief Introduction&quot;&gt;&lt;/a&gt;Brief Introduction&lt;/h3&gt;&lt;p&gt;Hi the</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title>Updated Modeling parameters for steel columns using IMK</title>
    <link href="http://example.com/2021/03/09/Updated-Modeling-parameters-for-steel-columns-using-IMK/"/>
    <id>http://example.com/2021/03/09/Updated-Modeling-parameters-for-steel-columns-using-IMK/</id>
    <published>2021-03-10T06:07:20.000Z</published>
    <updated>2021-03-10T06:27:10.479Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Updated-Steel-Column-Hinge-Modeling"><a href="#Updated-Steel-Column-Hinge-Modeling" class="headerlink" title="Updated Steel Column Hinge Modeling"></a>Updated Steel Column Hinge Modeling</h1><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference:"></a>Reference:</h2><p>[1]: Beam column hinge modeling summarized by GUAN</p><p>[2]: Proposed updates to the ASCE 41 nonlinear modeling parameters for wide-flange steel columns in support performance-based seismic engineering</p><h2 id="Background"><a href="#Background" class="headerlink" title="Background:"></a>Background:</h2><p>The file “Beam column hinge modeling summarized by GUAN” has summarized the findings for beam hinge modeling reported by Lignos. It says “until more experiments are conducted, the equations provided in the study could be used for columns”.</p><p>More recently, the modeling for <strong>column hinge</strong> has been updated and is summarized in this file.</p><p>Modified IMK considers three types of deteriorations: <strong>basic strength</strong>, <strong>post-peak strength</strong>, and <strong>unloading stiffness</strong>.</p><p>The <strong>reloading stiffness</strong> deterioration is not reflected.</p><h2 id="Equations-for-parameters"><a href="#Equations-for-parameters" class="headerlink" title="Equations for parameters"></a>Equations for parameters</h2><ul><li><p>Limitations are listed below:</p><p>Column section size is between <strong>W12</strong> to <strong>W36</strong>.</p><p>3.71 &lt;= h/t<sub>w</sub> &lt;= 57.5</p><p>38.4 &lt;= L<sub>b</sub>/r<sub>y</sub>  &lt;= 120</p><p>0.0 &lt;= P<sub>g</sub>/P<sub>ye</sub> &lt;= 0.75</p><p>Steel section is made up of <strong>ASTM A992 Gr. 50 Steel</strong>.</p></li><li><p>Pre-capping plastic rotation:</p><p><strong>Monotonic backbone</strong>:<br>$$<br>\theta_p = 294 \cdot (\frac{h}{t_w})^{-1.7} \cdot (\frac{L_b}{r_y})^{-0.7} \cdot (1-\frac{P_g}{P_{ye}})^{1.6} \leq 0.20\ rad<br>$$<br>First-cycle envelope:<br>$$<br>\theta_p^*=15 \cdot (\frac{h}{t_w})^{-1.6} \cdot (\frac{L_b}{r_y})^{-0.3} \cdot (1-\frac{P_g}{P_{ye}})^{2.3} \leq 0.10 rad<br>$$</p></li><li><p>Post-capping plastic rotation:</p><p><strong>Monotonic backbone</strong>:<br>$$<br>\theta_{pc} = 90 \cdot (\frac{h}{t_W})^{-0.8} \cdot (\frac{L_b}{r_y})^{-0.5} \cdot (1-\frac{P_g}{P_{ye}}) \leq 0.30\ rad<br>$$<br>First-cycle envelope:<br>$$<br>\theta_{pc}^* = 14 \cdot (\frac{h}{t_w})^{-0.8} \cdot (\frac{L_b}{r_y})^{-0.5} \cdot (1-\frac{P_g}{P_{ye}}) \leq 0.10\ rad<br>$$</p></li><li><p>Reference cumulative plastic rotation:<br>$$<br>\Lambda_s = 255000 \cdot (\frac{h}{t_w})^{-2.14} \cdot (\frac{L_b}{r_y})^{-0.53} \cdot (1-\frac{P_g}{P_{ye}})^{4.92}\leq \ 3.0,\quad \textrm{if}\ P_g/P_{ye} \leq \ 0.35 \</p><p>\Lambda_s = 268000 \cdot (\frac{h}{t_w})^{-2.30} \cdot (\frac{L_b}{r_y})^{-1.30} \cdot (1-P_g/P_{ye})^{1.19} \leq\ 3.0,\quad \textrm{if}\ P_g/P_{ye} &gt; 0.35<br>\</p><p>\Lambda_k = \Lambda_c = 0.9 \cdot \Lambda_s<br>$$</p></li><li><p>Effective yield strength:</p></li></ul><p>$$<br>M_y = M_y^* = 1.15 \cdot Z \cdot R_y \cdot F_{yn} \cdot (1-\frac{P_g}{2P_{ye}})\quad \textrm{if}\ P_g/P_{ye} &lt; 0.20<br>\<br>My = M_y^* = 1.15 \cdot Z \cdot Ry \cdot F_{yn} \cdot \frac{9}{8}(1-\frac{P_g}{P_{ye}})\quad \textrm{if}\ P_g/P_{ye} \geq 0.20<br>$$</p><ul><li><p>Residual strength ratio:</p><p><strong>Monotonic backbone</strong>:<br>$$<br>k = M_r/M_y = (0.5-0.4\cdot{P_g}{P_{ye}})<br>$$<br>First-cycle envelope:<br>$$<br>k = M_r^*/M_y^* = (0.4-0.4\cdot\frac{P_g}{P_ye})\cdot M_y^*<br>$$</p></li><li><p>Ultimate rotation capacity:</p><p><strong>Monotonic backbone</strong>:<br>$$<br>\theta_{ult} = 0.15<br>$$<br>First-cycle envelope:<br>$$<br>\theta_{ult}^* = 0.08 \cdot (1-0.6\cdot\frac{P_g}{P_{ye}})<br>$$</p></li><li><p>Peak flexural strength:</p><p><strong>Monotonic backbone</strong>:<br>$$<br>M_{max} = a \cdot M_y<br>\<br>a = 12.5 \cdot (\frac{h}{t_w})^{-0.2} \cdot (\frac{L_b}{r_y})^{-0.4} \cdot (1-\frac{P_g}{P_{ye}})^{0.4},\quad 1.0 \leq a \leq 1.3<br>$$<br>First-cycle envelope:<br>$$<br>a^* = 9.5 \cdot(\frac{h}{t_w})^{-0.4} \cdot (\frac{L_b}{r_y})^{-0.16} \cdot (1-\frac{P_g}{P_{ye}})^{0.2}\quad 1.0 \leq a \leq 1.3<br>$$</p></li><li><p>Effective yield rotation:<br>$$<br>\theta_y = M_y / K_e<br>$$</p></li><li><p>Elastic stiffness:<br>$$<br>K_e = L^2K_sK_b/[2(K_s+K_b)]<br>\<br>K_s = GA_w/L<br>\<br>K_b = 12EI/L^3<br>$$</p></li></ul><p>In all equations above, P<sub>g</sub> is the gravity-induced compressive load. P<sub>ye</sub> is the axial yield strength and is calculated based on expected steel material properties. F<sub>yn</sub> is the nominal yield stress of steel material.</p><h2 id="OpenSees-IMK-material-model"><a href="#OpenSees-IMK-material-model" class="headerlink" title="OpenSees IMK material model"></a>OpenSees IMK material model</h2>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;Updated-Steel-Column-Hinge-Modeling&quot;&gt;&lt;a href=&quot;#Updated-Steel-Column-Hinge-Modeling&quot; class=&quot;headerlink&quot; title=&quot;Updated Steel Column H</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title>Modeling parameters for steel beams using IMK</title>
    <link href="http://example.com/2021/03/09/Modeling-parameters-for-steel-beams-using-IMK/"/>
    <id>http://example.com/2021/03/09/Modeling-parameters-for-steel-beams-using-IMK/</id>
    <published>2021-03-10T05:50:40.000Z</published>
    <updated>2021-03-10T06:27:26.059Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Steel-Beam-Hinge-Modeling"><a href="#Steel-Beam-Hinge-Modeling" class="headerlink" title="Steel Beam Hinge Modeling"></a>Steel Beam Hinge Modeling</h1><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference:"></a>Reference:</h2><p>[1]: Hysteretic models that incorporate strength and stiffness deterioration</p><p>[2]: Deterioration modeling of steel components in support of collapse prediction of steel moment frames under earthquake loading</p><p>[3]: Global collapse of frame structures under seismic excitations</p><p>[4]: Sidesway collapse of deteriorating structural systems under seismic excitations</p><h2 id="Modeling-introduction"><a href="#Modeling-introduction" class="headerlink" title="Modeling introduction"></a>Modeling introduction</h2><ul><li><p>No deterioration exits:</p><p>Three parameters: initial stiffness K<sub>e</sub>; yield strength F<sub>y</sub>; strain-hardening stiffness K<sub>s</sub><br>$$<br>K_s = \alpha_sK_e<br>$$</p></li></ul><ul><li><p>Consider deterioration:</p><p>Apart from above three parameters; cap deformation <em>delta<sub>c</sub></em>; peak strength F<sub>c</sub>; Post-capping stiffness K<sub>c</sub>; residual strength F<sub>r</sub></p><p><img src="https://raw.githubusercontent.com/GUAN-XINGQUAN/images/main/img/20210309215331.png"><br>$$<br>K_s = \alpha_s K_e \<br>K_c = \alpha_c K_c \<br>F_r = \lambda F_y<br>$$</p></li></ul><p>Four types of deterioration involved in strength and stiffness deterioration: basic strength; post-capping strength, unloading stiffness, and accelerated reloading stiffness deteriorations.</p><p><img src="https://raw.githubusercontent.com/GUAN-XINGQUAN/images/main/img/20210309215401.png"></p><h2 id="Modeling-parameters"><a href="#Modeling-parameters" class="headerlink" title="Modeling parameters"></a>Modeling parameters</h2><p>Reference 2 provides a set of equations to calculate modeling parameters</p><p><img src="https://raw.githubusercontent.com/GUAN-XINGQUAN/images/main/img/20210309215429.png"></p><ul><li><p>Trend of modeling parameters</p><ul><li><p>Generally, modeling parameters for beams with other-than-RBS are smaller than that with RBS connection.</p></li><li><p>Dispersion of modeling parameters for beams with RBS connection are smaller.</p></li><li><p>Modeling parameters decrease as beam depth increases.</p></li><li><p>$\theta_p$ is linearly proportional to beam shear span L (distance from plastic hinge location to point of inflection)</p></li><li><p>Providing lateral bracing close to RBS portion of a beam decreases the rate of cyclic deterioration.</p></li><li><p>For most deep beam, small b<sub>f</sub>/2t<sub>f</sub> ratio has detrimental effect on theta_p, but benefits the parameters theta_pc and \LAMBDA.</p></li><li><p>h/t<sub>t</sub> is very important for all three modeling parameters (\theta_p, \theta_pc, \LAMBDA).</p></li></ul></li></ul><h2 id="Equations-for-parameters"><a href="#Equations-for-parameters" class="headerlink" title="Equations for parameters"></a>Equations for parameters</h2><ul><li><p>Database for the equations:</p><p>Set 1: Beams with other-than-RBS connections and depth 18 in. &lt;= d &lt;= 36 in.</p><p>Set 2: <strong>Beams with RBS connections and depth 18 in &lt;= d &lt;= 36 in</strong></p><p>Set 3: Beams with other-than-RBS connections and depth d &gt;= 21 in.</p><p>Set 4: Beams with RBS connections and depth &gt;= 21 in.</p><p><em>Sets 2 and 4 do not differ too much!!!</em></p></li></ul><ul><li><p>Pre-capping plastic rotation:</p><p>Data set 1:<br>$$<br>\theta_p = 0.0865 \cdot (\frac{h}{t_w})^{-0.365} \cdot (\frac{b_f}{2 \cdot t_f})^{-0.140} \cdot (\frac{L}{d})^{0.340} \cdot (\frac{c_{unit}^1 \cdot d}{533})^{-0.721} \cdot (\frac{c_{unit}^2 \cdot F_y}{355})^{-0.230}<br>$$<br>If millimeters and megapascals are used:<br>$$<br>c_{unit}^1 = c_{unit}^2 = 1.0<br>$$<br>If depth is in inches and F<sub>y</sub> is in ksi:<br>$$<br>c_{unit}^1 = 25.4\<br>c_{unit}^2 = 6.895<br>$$<br>Data set 3:<br>$$<br>\theta_p = 0.318 \cdot (\frac{h}{t_w})^{-0.550} \cdot (\frac{b_f}{2 \cdot t_f})^{-0.345} \cdot (\frac{L_b}{r_y})^{-0.0230} \cdot (\frac{L}{d})^{0.090} \cdot (\frac{c_{unit}^1 \cdot d}{533})^{-0.330} \cdot (\frac{c_{unit}^2 \cdot F_y}{355})^{-0.130}<br>$$<br><strong>Data set 2:</strong><br>$$<br>\theta_p = 0.19 \cdot (\frac{h}{t_w})^{-0.314} \cdot (\frac{b_f}{2 \cdot t_f})^{-0.100} \cdot (\frac{L_b}{r_y})^{-0.185} \cdot (\frac{L}{d})^{0.113} \cdot (\frac{c_{unit}^1 \cdot d}{533})^{-0.760} \cdot (\frac{c_{unit}^2 \cdot F_y}{355})^{-0.070}<br>$$</p></li></ul><ul><li><p>Post-capping plastic rotation:</p><p>Data set 1:<br>$$<br>\theta_{pc} = 5.63 \cdot (\frac{h}{t_w})^{-0.565} \cdot (\frac{b_f}{2 \cdot t_f})^{-0.800} \cdot (\frac{c_{unit}^1 \cdot d}{533})^{-0.280} \cdot (\frac{c_{unit}^2 \cdot F_y}{355})^{-0.430}<br>$$<br>Data set 3:<br>$$<br>\theta_{pc} = 7.50 \cdot (\frac{h}{t_w})^{-0.610} \cdot (\frac{b_f}{2 \cdot t_f})^{-0.710} \cdot (\frac{L_b}{r_y})^{-0.110} \cdot (\frac{c_{unit}^1 \cdot d}{533})^{-0.161} \cdot (\frac{c_{unit}^2 \cdot F_y}{355})^{-0.320}<br>$$<br><strong>Data set 2:</strong><br>$$<br>\theta_{pc} = 9.52 \cdot (\frac{h}{t_w})^{-0.513} \cdot (\frac{b_f}{2 \cdot t_f})^{-0.863} \cdot (\frac{L_b}{r_y})^{-0.108} \cdot (\frac{c_{unit}^2 \cdot F_y}{355})^{-0.360}<br>$$</p></li></ul><ul><li><p>Reference cumulative plastic rotation:</p><p>Data set 1:<br>$$<br>\Lambda = \frac{E_t}{M_y} = 495 \cdot (\frac{h}{t_w})^{-1.34} \cdot (\frac{b_f}{2 \cdot t_f})^{-0.595} \cdot (\frac{c_{unit}^2 \cdot F_y}{355})^{-0.360}<br>$$<br>Data set 3:<br>$$<br>\Lambda = \frac{E_t}{M_y} = 536 \cdot (\frac{h}{t_w})^{-1.26} \cdot (\frac{b_f}{2 \cdot t_f})^{-0.525} \cdot (\frac{L_b}{r_y})^{-0.130}\cdot (\frac{c_{unit}^2 \cdot F_y}{355})^{-0.291}<br>$$<br><strong>Data set 2:</strong><br>$$<br>\Lambda = \frac{E_t}{M_y} = 585 \cdot (\frac{h}{t_w})^{-1.14} \cdot (\frac{b_f}{2 \cdot t_f})^{-0.632} \cdot (\frac{L_b}{r_y})^{-0.205} \cdot (\frac{c_{unit}^2 \cdot F_y}{355})^{-0.391}<br>$$</p></li></ul><ul><li><p>Remarks:</p><p><em>The range of validity of these equations is only as good as the experimental data allows it to be.</em></p><p><em>Though the data does not include heavy W14 sections (heavier than W14X370) and heavy (heavier than W36X150) and deep (deeper than W36) beam sections. Predictions from regression equations were compared with existing heavy W14 sections and found to provide reasonable close values of experimentally obtained parameters.</em></p><p>**Until more experiments are conducted, the preceding equations provide the best estimates that can be offered to columns. **</p></li></ul><p>  <img src="https://raw.githubusercontent.com/GUAN-XINGQUAN/images/main/img/20210309215508.png"></p><p><img src="https://raw.githubusercontent.com/GUAN-XINGQUAN/images/main/img/20210309215526.png"></p><ul><li><p>Effective yield strength M<sub>y</sub>:</p><p>It is found to be slightly greater than predicted bending strength M<sub>y,p</sub>, which is defined as plastic section modulus Z times the measured material yield strength. For RBS, the ratio of M<sub>y</sub> to M<sub>y,p</sub> is 1.06, whereas for non-RBS is 1.17.</p></li></ul><p><img src="https://raw.githubusercontent.com/GUAN-XINGQUAN/images/main/img/20210309215558.png"></p><ul><li><p>Residual strength ratio k:</p><p><em>From the data sets for W-sections, a residual strength ratio k = M<sub>r</sub>/M<sub>y</sub> of approximately 0.4 is suggested for sets 3 and 4.</em></p><p><em>To assess it more reliably, more experiments with very large deformation cycles need to be conducted.</em></p></li></ul><ul><li><p>Ultimate rotation capacity $\theta_u$:</p><p>This quantity is highly dependent on loading history and my be very large for cases in which only a few very large cycles are executed.</p><p>Estimation regarding this parameter are made only for experiments with stepwise increasing cycles of the type required in AISC.</p><p>For other than RBS, it is 0.05 to 0.06 rad.</p><p><strong>For RBS, it is 0.06 to 0.07 rad.</strong></p><p>For monotonic loading, \theta_u is found to be on the order of three times as large as the \theta_u reported in symmetric cyclic loading protocols.</p></li></ul><h2 id="OpenSees-IMK-material-model"><a href="#OpenSees-IMK-material-model" class="headerlink" title="OpenSees IMK material model"></a>OpenSees IMK material model</h2><p>Reference: <a href="http://opensees.berkeley.edu/wiki/index.php/Modified_Ibarra-Medina-Krawinkler_Deterioration_Model_with_Bilinear_Hysteretic_Response_(Bilin_Material)">OpenSees IMK material model</a></p><ul><li>Elastic stiffness K<sub>0</sub>:<br>$$<br>K_0 = (n+1) \frac{6 \cdot E \cdot I_z}{L}<br>$$<br>where E, I and L are elastic modulus, moment of inertia, and length of beam. Typically, n is set as 10.</li></ul><ul><li><p>Strain hardening ratio for positive (negative) directions:</p><p><u>Will be computed at he very end of this note.</u></p></li></ul><ul><li><p>Effective yield strength M<sub>y</sub>:</p><p>According to findings reported in Ref [2], for beams with RBS connection:<br>$$<br>My = 1.06 \cdot M_{y,p} = 1.06 \cdot Z \cdot F_y<br>$$</p></li></ul><ul><li><p>Cyclic deterioration for <em>basic strength</em>, <em>post-capping strength</em>, <em>accelerated reloading stiffness</em>, and <em>unloading stiffness</em>.<br>$$<br>\begin{align}<br>&amp;\Lambda_S: \rm {basic\ strength}\<br>&amp;\Lambda_C: \rm {post-capping\ strength}\<br>&amp;\Lambda_A: \rm {accelerated\ reloading\ stiffness}\<br>&amp;\Lambda_K: \rm {unloading\ stiffness}<br>\end{align}<br>$$<br>A very large number means almost no cyclic deterioration.</p><p><strong>In my modeling</strong><br>$$<br>\begin{align}<br>&amp;\Lambda_K=\Lambda_A=\Lambda_S = \Lambda_C = (n+1) \cdot \Lambda = 11 \cdot \Lambda<br>\end{align}<br>$$<br>Where $\Lambda$ is calculated from regression equation in preceding section.</p></li></ul><ul><li><p>Rate of deterioration for <em>basic strength</em>, <em>post-capping strength</em>, <em>accelerated reloading</em>, and <em>unloading stiffness</em>.</p><p>By default: (<strong>In my modeling</strong>)<br>$$<br>\begin{align}<br>&amp; c_S = 1.0\<br>&amp; c_C = 1.0\<br>&amp; c_A = 1.0\<br>&amp; c_K = 1.0\<br>\end{align}<br>$$</p></li></ul><ul><li><p>Plastic rotation capacity <em>theta_p</em></p><p><strong>In my modeling</strong>, use the regression equation in the preceding section.</p></li></ul><ul><li><p>Post-capping rotation capacity <em>theta_pc</em>:</p><p><strong>In my modeling</strong>, use the regression equation in the preceding section.</p></li></ul><ul><li><p>Residual strength ratio <em>Res</em>:</p><p>Based on Ref [2], the ratio is assumed to be 0.4.</p></li></ul><ul><li><p>Ultimate rotation <em>theta_u</em>:</p><p>Based on preceding section and official OpenSees example, it is assumed to be 0.4. Ultimate rotation is associated with the failure of ductile tearing. Ref [4] reveals that ductile tearing will not be critical in most of cases.</p><p><strong>In my modeling</strong>, use 0.4.</p></li></ul><ul><li><p>Rate of cyclic deterioration <em>D</em> (for symmetric hysteretic response use 1.0):</p><p><strong>In my modeling</strong>, use 1.0.</p></li></ul><ul><li><p>elastic stiffness amplification factor <em>nFactor</em>:</p><p>This is optional, default value is 0.</p><p><strong>In my modeling</strong>, ignore this argument and the program would automatically use 0.</p></li></ul><ul><li><p><strong>Strain hardening ratio from Official OpenSees example</strong></p><p>For <em>elastic beam-column element</em>, modified moment of inertia is:<br>$$<br>I_{mod} = \frac{n+1.0}{n} \cdot I_z<br>$$<br>Rotational stiffness for <em>elastic beam-column element</em>:<br>$$<br>K_{bc} = \frac{n+1}{n} \cdot \frac{6 \cdot E \cdot I_z}{L}<br>$$</p><p>Initial stiffness for rotational spring for beam:</p></li></ul><p>$$<br>K_s = n \cdot \frac{6 \cdot E \cdot I_{mod}}{L} = (n+1.0) \cdot \frac{6 \cdot E \cdot I_z}{L}<br>$$</p><p>​    where I<sub>z</sub> is the moment of inertia for beam section.</p><p>​    Strain hardening ratio for spring:<br>$$<br>a_{mem} = (n+1.0) \cdot \frac{My \cdot (M_c/M_y - 1.0)}{\theta_p} \cdot \frac{1}{K_s}<br>$$</p><p>​    Modified strain hardening ratio for spring (<strong>used for modeling</strong>):<br>$$<br>b = \frac{a_{men}}{1+n \cdot (1 - a_{men})}<br>$$</p><ul><li><p>Strain hardening ratio from Prof. Burton’s codes:</p><p>Initial stiffness for rotational spring for beam:<br>$$<br>K = n_1 \cdot K_0 = 11 \cdot \frac{6 \cdot E \cdot I_z}{L}<br>$$<br>Straining hardening ratio for spring:<br>$$<br>asPosScaled = \frac{a_s}{1+n_2 \cdot (1- a_s)}<br>$$<br>Where n<sub>2</sub> = 10.</p><p>a<sub>s</sub> is calculated using the following equation:<br>$$<br>a_s = 0.11 \cdot \frac{My}{\theta_p} \cdot \frac{1}{K_0}\<br>{K_0} = \frac{6 \cdot E \cdot I_z}{L}<br>$$</p></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;Steel-Beam-Hinge-Modeling&quot;&gt;&lt;a href=&quot;#Steel-Beam-Hinge-Modeling&quot; class=&quot;headerlink&quot; title=&quot;Steel Beam Hinge Modeling&quot;&gt;&lt;/a&gt;Steel Beam </summary>
      
    
    
    
    <category term="Blog" scheme="http://example.com/categories/Blog/"/>
    
    
  </entry>
  
  <entry>
    <title>Week 2 Logistic Regression</title>
    <link href="http://example.com/2021/03/09/Week-2-Logistic-Regression/"/>
    <id>http://example.com/2021/03/09/Week-2-Logistic-Regression/</id>
    <published>2021-03-09T18:11:00.000Z</published>
    <updated>2021-03-09T18:15:23.592Z</updated>
    
    <content type="html"><![CDATA[<p>————————————— start from here ——————————————-</p><p>Background (问题背景):</p><p>Let’s consider a binary classification. For example, students are going to apply for student loan from a financial institution. The instution decides whether it will approve the student application based on two factors: the student’s credit score and his/her monthly income. Now we can use $X1$ to represent the student’s monthly income and $X2$ to represent the student’s credit score. Then we can use $Y \in {1,0}$ to denote the application results. $Y_i=1$ means that the institution approves the student’s application and 0 for the reject of application.</p><p>Imagine that we have 100 students who will apply for the student loan. Obviously, some of them can get the load whereas others get rejected.</p><p>To model this problem, we can use logistic regression to classify the students who can and who cannot get the loan. $X1$ and $X2$ are two column vectors with the length of 100. Y is a column vector whose length is also 100. Each element in Y can either be 1 or 0.</p><p>我们现在来考虑一个二分类问题。例如，每年都会有学生向银行申请学生贷款。一般而言，银行会根据两个因素来判定是否通过该学生贷款的申请：信用记录分数以及该学生每个月的收入。这样的话，我们可以用$X1$和$X2$分别表达学生的月收入和信用分数。然后我们可以用$Y \in {1,0}$来表达该学生是否可以成功从银行取得贷款。Y=1表示该学生成功获得贷款，而Y=0表示该生贷款申请遭到拒绝。</p><p>假设我们有200个学生向银行申请贷款。显然，这200名学生中有人将取得贷款，而另一部分学生可能因为较差的信用分数或者较低的月收入而申请被拒。</p><p>我们可以利用逻辑回归对该问题建模。这里，$X1$和$X2$分别是一个长度为100的列向量。Y也是一个长度为200的列向量。Y中的每一个元素只可能是1或者0.</p><p>First of all, the data of X1, X2, and Y are generated using some random function.</p><p>首先我们来生成X1, X2, 和Y的数据。</p><p>(It should be noted that the generation for X1, X2, and Y are kind of arbitrary as we don’t have actual data from financial institution)</p><p>这里需要声明的是，X1, X2以及Y的是随意生成的，因为我们没有实际的数据。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Add this command because of plot</span></span><br><span class="line"><span class="comment"># Otherwise, the plot may not show up in Anaconda</span></span><br><span class="line">% matplotlib inline</span><br><span class="line"></span><br><span class="line"><span class="comment"># Import necessary packages</span></span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt  <span class="comment"># Package used for plotting the figure</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np  <span class="comment"># Package relating to math</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Define length of X1 vector, which is dentical to the length of X2 and Y</span></span><br><span class="line"><span class="comment"># In total, 200 students are applying for the student load</span></span><br><span class="line">number_observation = <span class="number">200</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Generate X1 and X2</span></span><br><span class="line">X1 = np.linspace(<span class="number">1600</span>, <span class="number">2500</span>, number_observation)  <span class="comment"># Assume the students&#x27; monthly income ranges from 1600 to 2500</span></span><br><span class="line">X2 = X1/<span class="number">2.5</span> + np.random.normal(<span class="number">0</span>, <span class="number">100</span>, number_observation)  <span class="comment"># Assume the credit scrore is following this function</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Generate Y vector</span></span><br><span class="line">Y = np.linspace(<span class="number">0</span>, <span class="number">0</span>, number_observation)</span><br><span class="line"><span class="comment"># indicator represents whther the financial institution approves the student&#x27;s application or not</span></span><br><span class="line"><span class="comment"># indicator &lt; 0 means the application is rejected.</span></span><br><span class="line"><span class="comment"># indicator &gt;= 0 means the applcation is approved.</span></span><br><span class="line"><span class="comment"># A noise term is added because I don&#x27;t want the data to be linearly saparable</span></span><br><span class="line">indicator = (X1 - np.mean(X1))/np.std(X1) + (X2 - np.mean(X2))/np.std(X2) + np.random.normal(<span class="number">0</span>, <span class="number">0.25</span>, number_observation)</span><br><span class="line">Y[indicator &lt; <span class="number">0</span>] = <span class="number">0</span></span><br><span class="line">Y[indicator &gt;= <span class="number">0</span>] = <span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Visualize the data points</span></span><br><span class="line">plt.figure(figsize=(<span class="number">16</span>,<span class="number">8</span>))</span><br><span class="line">plt.scatter(X1[indicator &lt; <span class="number">0</span>], X2[indicator &lt; <span class="number">0</span>], color=<span class="string">&#x27;b&#x27;</span>, marker=<span class="string">&#x27;o&#x27;</span>, label=<span class="string">&#x27;Rejected&#x27;</span>)</span><br><span class="line">plt.scatter(X1[indicator &gt;= <span class="number">0</span>], X2[indicator &gt;= <span class="number">0</span>], color=<span class="string">&#x27;r&#x27;</span>, marker=<span class="string">&#x27;*&#x27;</span> , label=<span class="string">&#x27;Approved&#x27;</span>)</span><br><span class="line">font = &#123;<span class="string">&#x27;family&#x27;</span>:<span class="string">&#x27;Times New Roman&#x27;</span>,<span class="string">&#x27;weight&#x27;</span> : <span class="string">&#x27;normal&#x27;</span>,<span class="string">&#x27;size&#x27;</span>: <span class="number">16</span>,&#125;</span><br><span class="line">plt.xlabel(<span class="string">&#x27;X1&#x27;</span>, font)</span><br><span class="line">plt.ylabel(<span class="string">&#x27;X2&#x27;</span>, font)</span><br><span class="line">plt.xticks(fontsize=<span class="number">16</span>, fontname=<span class="string">&#x27;Times New Roman&#x27;</span>)</span><br><span class="line">plt.yticks(fontsize=<span class="number">16</span>, fontname=<span class="string">&#x27;Times New Roman&#x27;</span>)</span><br><span class="line">plt.title(<span class="string">&#x27;The distibution of X1 and X2&#x27;</span>, font)</span><br><span class="line">plt.legend(fontsize=<span class="number">16</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="https://raw.githubusercontent.com/GUAN-XINGQUAN/images/main/img/20210309101334.png"></p><p>Later we will use sigmoid function to deal with these data points. As we know, sigmoid function invovles the exponential algorithm. Since X1 is large (the maximum is about 2400), we may encounter overflow problem in coding. To avoid the overflow problem, we have to normailize X1 and X2.</p><p>在后面我们将会用sigmoid函数。这个函数包含了以自然指数e为底的运算。因为X1的数值很大，因此我们很有可能会碰到数值溢出的问题。为了避免该类问题，我们需要将X1和X2正则化。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Normalize X1 and X2</span></span><br><span class="line">new_X1 = (X1 - np.mean(X1))/np.std(X1)</span><br><span class="line">new_X2 = (X2 - np.mean(X2))/np.std(X2)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Visualize the data points</span></span><br><span class="line">plt.figure(figsize=(<span class="number">16</span>,<span class="number">8</span>))</span><br><span class="line">plt.scatter(new_X1[indicator &lt; <span class="number">0</span>], new_X2[indicator &lt; <span class="number">0</span>], color=<span class="string">&#x27;b&#x27;</span>, marker=<span class="string">&#x27;o&#x27;</span>, label=<span class="string">&#x27;Rejected&#x27;</span>)</span><br><span class="line">plt.scatter(new_X1[indicator &gt;= <span class="number">0</span>], new_X2[indicator &gt;= <span class="number">0</span>], color=<span class="string">&#x27;r&#x27;</span>, marker=<span class="string">&#x27;*&#x27;</span> , label=<span class="string">&#x27;Approved&#x27;</span>)</span><br><span class="line">font = &#123;<span class="string">&#x27;family&#x27;</span>:<span class="string">&#x27;Times New Roman&#x27;</span>,<span class="string">&#x27;weight&#x27;</span> : <span class="string">&#x27;normal&#x27;</span>,<span class="string">&#x27;size&#x27;</span>: <span class="number">16</span>,&#125;</span><br><span class="line">plt.xlabel(<span class="string">&#x27;new X1&#x27;</span>, font)</span><br><span class="line">plt.ylabel(<span class="string">&#x27;new X2&#x27;</span>, font)</span><br><span class="line">plt.xticks(fontsize=<span class="number">16</span>, fontname=<span class="string">&#x27;Times New Roman&#x27;</span>)</span><br><span class="line">plt.yticks(fontsize=<span class="number">16</span>, fontname=<span class="string">&#x27;Times New Roman&#x27;</span>)</span><br><span class="line">plt.title(<span class="string">&#x27;The distibution of new X1 and X2&#x27;</span>, font)</span><br><span class="line">plt.legend(fontsize=<span class="number">16</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="https://raw.githubusercontent.com/GUAN-XINGQUAN/images/main/img/20210309101349.png"></p><p>As we can see from the figure, the process of normalization only affects the values of X1 and X2. It doesn’t affect the distribution of X1 and X2 (the layout of X1 and X2 are still the same).</p><p>从上图可以看出，对X1和X2正则化只是影响了横纵坐标的数值大小，并不会影响数据的分布趋势。</p><p>In the figure, red stars represent the case that a student’s loan application is approved, whereas the blue dots represent the case that a student’s loan application is declined.</p><p>图中，红色五角星表示学生的贷款申请被批准；蓝色圆心表示学生的贷款申请被拒。</p><p>Although the data points are generated in an arbitrary method, these data points are consistent with our common sense. A student with higher monthly income and higher credit score tends to get approved for his appliation. In contrast, a student with low income and low credit score tends to be rejected for applying the loan.</p><p>尽管这些数据点是随意生成的，但是其符合我们的正常认知。当一个学生有较高的月收入和较高的信用分数，他将会大概率获得贷款。而当一个学生的月收入较低，信用分数也很低时，他的贷款申请将会被拒。</p><p>Now, we would like to use the sigmoid function to quantify the possibility (or probability) for a single student to get the loan.</p><p>现在我们需要用下面的这个叫做sigmoid的函数来表征单个学生能够拿到贷款的概率:</p><p>$h^i(X_1, X_2) = \frac{e^{\beta_1 X_1^i + \beta_2 X_2^i}}{1 + e^{\beta_1 X_1^i + \beta_2 X_2^i}}$</p><p>where the superscript $i$ represents the $i^{th}$ student.</p><p>式中上标$i$表示第$i$个学生。</p><p>The question is then how to determine the two parameters: $\beta_1$ and $\beta_2$</p><p>现在的问题便是如何确定$\beta_1$和$\beta_2$的数值。</p><p>As what we did in week 1 Linear regression, we need to introduce an objective function (loss function). Then we use gradient descent to find the global minimum of the objective function.</p><p>正如我们在第一周线性回归中讲述的一样，我们需要引入一个目标函数（损失函数）。然后我们利用梯度下降方法来寻找目标函数的最小值。</p><p>The objective function is listed below and it is revised from the maximum likelihood function of logistic regression:</p><p>目标函数定义如下，它是基于逻辑回归的最大似然函数修改得到的：</p><p>$L(\beta_1, \beta_2) = -\frac{1}{N}\sum_{i=0}^N{y^i ln[h^i(X_1^i, X_2^i)]+(1-y^i) ln[1-h(X_1^i, X_2^i)]}$</p><p>The plot for the objective function is shown below:</p><p>该目标函数图象如下所示：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Import the package for three-dimensional plot</span></span><br><span class="line"><span class="keyword">from</span> mpl_toolkits.mplot3d <span class="keyword">import</span> Axes3D</span><br><span class="line"></span><br><span class="line"><span class="comment"># Generate values for beta_1 and beta_2</span></span><br><span class="line">number_beta = <span class="number">100</span></span><br><span class="line">beta_1 = np.linspace(<span class="number">2</span>, <span class="number">8</span>, number_beta)</span><br><span class="line">beta_2 = np.linspace(<span class="number">1</span>, <span class="number">8</span>, number_beta)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Initialize a matrix to store loss function values</span></span><br><span class="line">loss = np.zeros([number_beta, number_beta])</span><br><span class="line"></span><br><span class="line"><span class="comment"># Initialize a matrix to store grid values</span></span><br><span class="line">grid_beta_1 = np.zeros([number_beta, number_beta])</span><br><span class="line">grid_beta_2 = np.zeros([number_beta, number_beta])</span><br><span class="line"></span><br><span class="line"><span class="comment"># Calculate the loss function values under different beta_1 and beta_2</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(number_beta):</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(number_beta):</span><br><span class="line">        grid_beta_1[i,j] = beta_1[i]</span><br><span class="line">        grid_beta_2[i,j] = beta_2[j]</span><br><span class="line">        s = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> indx <span class="keyword">in</span> <span class="built_in">range</span>(number_observation):</span><br><span class="line">            h = np.exp(new_X1[indx] * beta_1[i] + new_X2[indx] * beta_2[j]) / (<span class="number">1</span>+np.exp(new_X1[indx] * beta_1[i] + new_X2[indx] * beta_2[j]))</span><br><span class="line">            temp = Y[indx]*np.log(h) + (<span class="number">1</span>-Y[indx])*np.log(<span class="number">1</span>-h)</span><br><span class="line">            s = s + temp</span><br><span class="line">        loss[i,j] = -<span class="number">1</span>/number_observation * s</span><br><span class="line">        </span><br><span class="line"><span class="comment"># Plot the loss function values over the beta_0 and beta_1 values</span></span><br><span class="line">fig = plt.figure(figsize=(<span class="number">16</span>,<span class="number">8</span>))</span><br><span class="line">ax = Axes3D(fig)</span><br><span class="line">ax.set_xlabel(<span class="string">r&quot;$\beta_1$&quot;</span>, font)</span><br><span class="line">ax.set_ylabel(<span class="string">r&quot;$\beta_2$&quot;</span>, font)</span><br><span class="line">ax.set_zlabel(<span class="string">&quot;Loss function value&quot;</span>, font)</span><br><span class="line">plt.xticks(fontsize=<span class="number">16</span>, fontname=<span class="string">&#x27;Times New Roman&#x27;</span>)</span><br><span class="line">plt.yticks(fontsize=<span class="number">16</span>, fontname=<span class="string">&#x27;Times New Roman&#x27;</span>)</span><br><span class="line">ax.plot_surface(grid_beta_1, grid_beta_2, loss, cmap=<span class="string">&#x27;rainbow&#x27;</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="https://raw.githubusercontent.com/GUAN-XINGQUAN/images/main/img/20210309101419.png"></p><p>From the figure above, the objective function has a global minimum and we are trying to find this global minimum. Again we will use gradient descent to reach our target.</p><p>从上图可以看出，目标函数是有全局最小值的。我们会用梯度下降的方法来寻找该最小值。</p><p>Let’s calculate the derivate for the objective function.</p><p>先来计算目标函数关于$\beta_1$和$\beta_2$的导数。</p><p>$\frac{\partial L(\beta_1,\beta_2)}{\partial \beta_1}= -\frac{1}{N}\sum_{i=1}^N {y^i \frac{1}{h^i} (h^i)’ + (1-y^i \frac{1}{1-h^i} (-h^i)’}$</p><p>$=-\frac{1}{N}\sum_{i=1}^N {\frac{y^i}{h^i} - \frac{(1-y^i)}{1-h^i}} (h^i)’$</p><p>$=-\frac{1}{N}\sum_{i=1}^N {\frac{y^i}{h^i} - \frac{(1-y^i)}{1-h^i}} h^i (1-h^i) x_1^i$</p><p>$=-\frac{1}{N}\sum_{i=1}^N {y^i - h^i}x_1^i$</p><br><p>$\frac{\partial L(\beta_1,\beta_2)}{\partial \beta_2}= -\frac{1}{N}\sum_{i=1}^N {y^i \frac{1}{h^i} (h^i)’ + (1-y^i \frac{1}{1-h^i} (-h^i)’}$</p><p>$=-\frac{1}{N}\sum_{i=1}^N {\frac{y^i}{h^i} - \frac{(1-y^i)}{1-h^i}} (h^i)’$</p><p>$=-\frac{1}{N}\sum_{i=1}^N {\frac{y^i}{h^i} - \frac{(1-y^i)}{1-h^i}} h^i (1-h^i) x_2^i$</p><p>$=-\frac{1}{N}\sum_{i=1}^N {y^i - h^i}x_2^i$</p><p>Where the superscript $i$ means the $i^{th}$ element in X1, X2, or Y. $h$ is just shorthand for $h(X1, X2)$</p><p>上式中，上标$i$表示X1，X2或者Y中的第$i$个元素。$h$是$h(X1,X2)$的简写。</p><p>Then we can use the negative gradient direction to update the two parameters:</p><p>接下来我们可以用负梯度方向来更新两个参数：</p><p>$\beta_1^{n+1} = \beta_1^n - s\frac{\partial L(\beta_1^n, \beta_2^n)}{\partial \beta_1^n}$</p><p>$\beta_2^{n+1} = \beta_2^n - s\frac{\partial L(\beta_1^n, \beta_2^n)}{\partial \beta_2^n}$</p><p>First of all, let’s arbitrarily give two initial values to $\beta_1$ and $\beta_2$ and visualize the classification boundary.</p><p>首先我们先给$\beta_1$和$\beta_2$任意赋两个初始值，然后画出区分边界。</p><p>The boundary line is drawn using the following equation:</p><p>边界的直线由下面的等式来确定：</p><p>$\beta_1 xx + \beta_2 yy = 0$ </p><p>It should be noted that xx and yy here only refers to the coordinates of horizontal or vertical axes. They don’t refer to any predictor or response mentioned in the backgroun section. They are used for plotting the boundary only. No special meanings! Don’t get confused! $\beta_1$ and $\beta_2$ are two parameters of our interest, both of which are obtained from gradient descent algorithm.</p><p>需要注意的是，上面等式里面的xx和yy仅仅指二维坐标系中的坐标值，仅作画边界直线用，与问题背景交代中的x和y没有任何关系。不要混淆了。$\beta_1$和$\beta_2$是我们所感兴趣的参数，由梯度下降得到。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Two arbitrary values for beta_1 and beta_2</span></span><br><span class="line">beta_1 = <span class="number">0.1</span></span><br><span class="line">beta_2 = <span class="number">0.6</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Get the boundary</span></span><br><span class="line">xx = np.linspace(-<span class="number">2</span>, <span class="number">2</span>, <span class="number">100</span>)</span><br><span class="line">yy = -beta_1 * xx / beta_2</span><br><span class="line"></span><br><span class="line"><span class="comment"># Plot the data points and boundary line at initial beta_0 and beta_1</span></span><br><span class="line">fig = plt.figure(figsize=(<span class="number">16</span>,<span class="number">8</span>))</span><br><span class="line">plt.scatter(new_X1[indicator &lt; <span class="number">0</span>], new_X2[indicator &lt; <span class="number">0</span>], color=<span class="string">&#x27;b&#x27;</span>, marker=<span class="string">&#x27;o&#x27;</span>, label=<span class="string">&#x27;Rejected&#x27;</span>)</span><br><span class="line">plt.scatter(new_X1[indicator &gt;= <span class="number">0</span>], new_X2[indicator &gt;= <span class="number">0</span>], color=<span class="string">&#x27;r&#x27;</span>, marker=<span class="string">&#x27;*&#x27;</span> , label=<span class="string">&#x27;Approved&#x27;</span>)</span><br><span class="line">plt.plot(xx, yy, label=<span class="string">&#x27;Boundary&#x27;</span>)</span><br><span class="line">font = &#123;<span class="string">&#x27;family&#x27;</span>:<span class="string">&#x27;Times New Roman&#x27;</span>,<span class="string">&#x27;weight&#x27;</span> : <span class="string">&#x27;normal&#x27;</span>,<span class="string">&#x27;size&#x27;</span>: <span class="number">16</span>,&#125;</span><br><span class="line">plt.xlabel(<span class="string">&#x27;new X1&#x27;</span>, font)</span><br><span class="line">plt.ylabel(<span class="string">&#x27;new X2&#x27;</span>, font)</span><br><span class="line">plt.xticks(fontsize=<span class="number">16</span>, fontname=<span class="string">&#x27;Times New Roman&#x27;</span>)</span><br><span class="line">plt.yticks(fontsize=<span class="number">16</span>, fontname=<span class="string">&#x27;Times New Roman&#x27;</span>)</span><br><span class="line">plt.title(<span class="string">&#x27;The distibution of new X1 and X2&#x27;</span>, font)</span><br><span class="line">plt.legend(fontsize=<span class="number">16</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="https://raw.githubusercontent.com/GUAN-XINGQUAN/images/main/img/20210309101444.png"></p><p>Not surprisingly, the line does not classify two types of data points well.</p><p>随意给出的这条直线并不能很好的划分两类数据点。</p><p>Now let’s use gradient descent to update $\beta_1$ and $\beta_2$.</p><p>现在用梯度下降的方法来更新$\beta_1$和$\beta_2$的数值：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Assume we are going to iterate 1000 times</span></span><br><span class="line">max_iteration = <span class="number">3000</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Define step size</span></span><br><span class="line">step_size = <span class="number">0.0001</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Initialize beta_1, beta_2, and loss to track the update algorithm</span></span><br><span class="line">beta_1_update = np.zeros([max_iteration+<span class="number">1</span>, <span class="number">1</span>])</span><br><span class="line">beta_2_update = np.zeros([max_iteration+<span class="number">1</span>, <span class="number">1</span>])</span><br><span class="line">loss_update = np.zeros([max_iteration+<span class="number">1</span>, <span class="number">1</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># Initial value for beta_1 and beta_2</span></span><br><span class="line">beta_1_update[<span class="number">0</span>,<span class="number">0</span>] = beta_1</span><br><span class="line">beta_2_update[<span class="number">0</span>,<span class="number">0</span>] = beta_2</span><br><span class="line"></span><br><span class="line"><span class="comment"># Iterate many times in order to get the best values for beta_0 and beta_1</span></span><br><span class="line"><span class="keyword">for</span> <span class="built_in">iter</span> <span class="keyword">in</span> <span class="built_in">range</span>(max_iteration):</span><br><span class="line">    <span class="comment"># Calculate h function results</span></span><br><span class="line">    h = np.exp(new_X1 * beta_1_update[<span class="built_in">iter</span>,<span class="number">0</span>] + new_X2 * beta_2_update[<span class="built_in">iter</span>,<span class="number">0</span>]) / (<span class="number">1</span> + np.exp(new_X1 * beta_1_update[<span class="built_in">iter</span>,<span class="number">0</span>] + new_X2 * beta_2_update[<span class="built_in">iter</span>,<span class="number">0</span>]))</span><br><span class="line">    <span class="comment"># Calculate the loss function value</span></span><br><span class="line">    loss_update[<span class="built_in">iter</span>,<span class="number">0</span>] = -np.mean(Y * np.log(h) + (<span class="number">1</span>-Y) * np.log(<span class="number">1</span>-h))</span><br><span class="line">    <span class="comment"># Calculate the gradient</span></span><br><span class="line">    gradient_beta_1 = -np.<span class="built_in">sum</span>((Y-h) * X1)/number_observation</span><br><span class="line">    gradient_beta_2 = -np.<span class="built_in">sum</span>((Y-h) * X2)/number_observation</span><br><span class="line">    <span class="comment"># Update beta_1 and beta_2</span></span><br><span class="line">    beta_1_update[<span class="built_in">iter</span>+<span class="number">1</span>] = beta_1_update[<span class="built_in">iter</span>] - step_size * gradient_beta_1</span><br><span class="line">    beta_2_update[<span class="built_in">iter</span>+<span class="number">1</span>] = beta_2_update[<span class="built_in">iter</span>] - step_size * gradient_beta_2</span><br><span class="line"></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Plot the loss vs. iteration times</span></span><br><span class="line">plt.figure(figsize=(<span class="number">16</span>,<span class="number">8</span>))</span><br><span class="line">plt.plot(<span class="built_in">range</span>(max_iteration), loss_update[<span class="number">0</span>:max_iteration,<span class="number">0</span>], color=<span class="string">&#x27;g&#x27;</span>, linewidth=<span class="number">2.0</span>, label=<span class="string">&#x27;fitted line&#x27;</span>)</span><br><span class="line">font = &#123;<span class="string">&#x27;family&#x27;</span>:<span class="string">&#x27;Times New Roman&#x27;</span>,<span class="string">&#x27;weight&#x27;</span> : <span class="string">&#x27;normal&#x27;</span>,<span class="string">&#x27;size&#x27;</span>: <span class="number">16</span>,&#125;</span><br><span class="line">plt.xlabel(<span class="string">&#x27;iteration times&#x27;</span>, font)</span><br><span class="line">plt.ylabel(<span class="string">&#x27;loss function values&#x27;</span>, font)</span><br><span class="line">plt.xticks(fontsize=<span class="number">16</span>, fontname=<span class="string">&#x27;Times New Roman&#x27;</span>)</span><br><span class="line">plt.yticks(fontsize=<span class="number">16</span>, fontname=<span class="string">&#x27;Times New Roman&#x27;</span>)</span><br><span class="line">plt.title(<span class="string">&#x27;Loss function value vs. iteration times&#x27;</span>, font)</span><br><span class="line">plt.legend(fontsize=<span class="number">16</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="https://raw.githubusercontent.com/GUAN-XINGQUAN/images/main/img/20210309101500.png"></p><p>As shown in the figure above, the loss already reaches convergence, which means increasing the iteration times would not further decrease the loss.</p><p>从上图可以看出，损失值已达到收敛。这意味着进一步增加迭代次数不会减少损失值。</p><p>The final boundary and data points are shown below:</p><p>最终的边界如下图所示：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Plot the data points and boundary line at final beta_0 and beta_1</span></span><br><span class="line">xx = np.linspace(-<span class="number">2</span>, <span class="number">2</span>, <span class="number">100</span>)</span><br><span class="line">yy = -beta_1_update[-<span class="number">1</span>,<span class="number">0</span>] * xx / beta_2_update[-<span class="number">1</span>,<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">fig = plt.figure(figsize=(<span class="number">16</span>,<span class="number">8</span>))</span><br><span class="line">plt.scatter(new_X1[indicator &lt; <span class="number">0</span>], new_X2[indicator &lt; <span class="number">0</span>], color=<span class="string">&#x27;b&#x27;</span>, marker=<span class="string">&#x27;o&#x27;</span>, label=<span class="string">&#x27;Rejected&#x27;</span>)</span><br><span class="line">plt.scatter(new_X1[indicator &gt;= <span class="number">0</span>], new_X2[indicator &gt;= <span class="number">0</span>], color=<span class="string">&#x27;r&#x27;</span>, marker=<span class="string">&#x27;*&#x27;</span> , label=<span class="string">&#x27;Approved&#x27;</span>)</span><br><span class="line">plt.plot(xx, yy, label=<span class="string">&#x27;Final boundary&#x27;</span>)</span><br><span class="line">font = &#123;<span class="string">&#x27;family&#x27;</span>:<span class="string">&#x27;Times New Roman&#x27;</span>,<span class="string">&#x27;weight&#x27;</span> : <span class="string">&#x27;normal&#x27;</span>,<span class="string">&#x27;size&#x27;</span>: <span class="number">16</span>,&#125;</span><br><span class="line">plt.xlabel(<span class="string">&#x27;new X1&#x27;</span>, font)</span><br><span class="line">plt.ylabel(<span class="string">&#x27;new X2&#x27;</span>, font)</span><br><span class="line">plt.xticks(fontsize=<span class="number">16</span>, fontname=<span class="string">&#x27;Times New Roman&#x27;</span>)</span><br><span class="line">plt.yticks(fontsize=<span class="number">16</span>, fontname=<span class="string">&#x27;Times New Roman&#x27;</span>)</span><br><span class="line">plt.title(<span class="string">&#x27;The distibution of new X1 and X2&#x27;</span>, font)</span><br><span class="line">plt.legend(fontsize=<span class="number">16</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="https://raw.githubusercontent.com/GUAN-XINGQUAN/images/main/img/20210309101520.png"></p><p>As shwon in the figure above, the boundary separates the blue dots and red stars. Although there are some mis-classification points, these mis-classifications are inevitable as the original data points are not linear separable.</p><p>如上图所示，最终的边界直线可以较好的区分蓝色圆点和红色星状点。尽管有一些数据点没有被正确的划分，这种“错误”是无法避免的，因为我们原始的数据并不是被线性函数完全分割的(即两类数据点相互侵入对方区域)。</p><p>———————————– end —————————————————-</p><p>Review (往期回顾):</p><p>Week 1 Linear regression (线性回归)<br><a href="https://blog.csdn.net/weixin_42515443/article/details/80768841">https://blog.csdn.net/weixin_42515443/article/details/80768841</a></p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;————————————— start from here ——————————————-&lt;/p&gt;
&lt;p&gt;Background (问题背景):&lt;/p&gt;
&lt;p&gt;Let’s consider a binary classification. For example, stude</summary>
      
    
    
    
    <category term="Blog" scheme="http://example.com/categories/Blog/"/>
    
    
  </entry>
  
  <entry>
    <title>Week 1 Linear Regression</title>
    <link href="http://example.com/2021/03/08/Week-1-Linear-Regression/"/>
    <id>http://example.com/2021/03/08/Week-1-Linear-Regression/</id>
    <published>2021-03-09T07:34:05.000Z</published>
    <updated>2021-03-09T18:30:47.749Z</updated>
    
    <content type="html"><![CDATA[<p>I have been learning the statistics for about one year. Nine months ago, I didn’t believe that I can survive from statistics classes as I don’t know much about statistics. For example, I don’t even know that Gaussian distribution is another name of Normal distribution. I just want write something here to summarize what I have learned during this year.</p><p>统计学也陆陆续续的学了一年了。没想到自己除了土木工程还是可以学一些其他的东西。现在利用暑假总结一下自己这一年学过的统计学知识，也算是对自己统计学的辅修有一个交代。</p><p>What I am writing here is something I summarized using my own words. These “words” may not be accurate enough as I am an Engineering student, not a student majoring in statistics. I just wrote all this stuff based on my own interest. I hope this note could help someone who doesn’t has much background in statistics but wants to learn some statistics.</p><p>这些所写的内容都是基于我自己的理解，里面所用到的一些词语并不是非常准确。毕竟是我只是一个工程学科的学生，不是一个统计学学生。我写下这些东西也只是基于自己的兴趣爱好，同时希望帮助一些像我一样零基础的同学学习统计学。</p><p>———————————————– starts from here ———————————————————</p><p>Background (问题背景):</p><p>Assume that both X and Y are a N by 1 vector. X is called as predictor and Y is called as response.</p><p>假定X和Y均是一个N行1列的向量（矩阵）。X被称作自变量，Y被称作因变量。</p><p>The relationship between X and Y is presented in the figure below (python code is provided):</p><p>X与Y之间的关系见下图所示 (python源代码也附在下面)：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Add this command in order to show the plot in Anaconda</span></span><br><span class="line"><span class="comment"># Otherwise the plot may not show up</span></span><br><span class="line">% matplotlib inline</span><br><span class="line"></span><br><span class="line"><span class="comment"># Import necessary packages</span></span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt  <span class="comment"># Package used for plotting the figure</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np  <span class="comment"># Package for matrix operation</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Define the number of observation</span></span><br><span class="line"><span class="comment"># This number is also the length of X or Y vector</span></span><br><span class="line">number_observation = <span class="number">1000</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Generate X using linear space, i.e., X increase from -5 to 5. The interval is a constant such that there will be 1000 X.</span></span><br><span class="line">X = np.linspace(-<span class="number">5</span>, <span class="number">5</span>, number_observation)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Generate Y</span></span><br><span class="line"><span class="comment"># Assume Y is generated using the equation: Y = 2X + 3 + e</span></span><br><span class="line"><span class="comment"># e refers to the noise, which may be caused by measurement error.</span></span><br><span class="line"><span class="comment"># e is assumed to follow normal distribution with a mean of 0 and a standard deviation of 1</span></span><br><span class="line">e = np.random.normal(<span class="number">0</span>, <span class="number">1</span>, number_observation)</span><br><span class="line">Y = <span class="number">2</span>*X + <span class="number">3</span> + e</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Plot the X and Y</span></span><br><span class="line">plt.figure(figsize=(<span class="number">16</span>,<span class="number">8</span>))</span><br><span class="line">plt.scatter(X, Y, color=<span class="string">&#x27;r&#x27;</span>)</span><br><span class="line">font = &#123;<span class="string">&#x27;family&#x27;</span>:<span class="string">&#x27;Times New Roman&#x27;</span>,<span class="string">&#x27;weight&#x27;</span> : <span class="string">&#x27;normal&#x27;</span>,<span class="string">&#x27;size&#x27;</span>: <span class="number">16</span>,&#125;</span><br><span class="line">plt.xlabel(<span class="string">&#x27;X&#x27;</span>, font)</span><br><span class="line">plt.ylabel(<span class="string">&#x27;Y&#x27;</span>, font)</span><br><span class="line">plt.xticks(fontsize=<span class="number">16</span>, fontname=<span class="string">&#x27;Times New Roman&#x27;</span>)</span><br><span class="line">plt.yticks(fontsize=<span class="number">16</span>, fontname=<span class="string">&#x27;Times New Roman&#x27;</span>)</span><br><span class="line">plt.title(<span class="string">&#x27;The relationship between X and Y&#x27;</span>, font)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="https://raw.githubusercontent.com/GUAN-XINGQUAN/images/main/img/20210308231915.png"></p><p>Now we roughly know the relationship between X and Y. It seems that Y is linearly dependent on X. Therefore we try to use a linear function to fit these data points.</p><p>现在我们大致了解X与Y之间的关系。通过图可以看出，Y似乎和X呈现出一种线性关系。因此我们需要用线性函数来拟合这些离散的数据点。</p><p>The linear function is presented below:</p><p>线性函数的形式如下所示：</p><p>$h(x_i)=\beta_1 X_i + \beta_0$</p><p>Where $h(x_i)$ is the predicted response when $x_i$ is given.</p><p>函数中，$h(x_i)$是当给定$X_i$值时，预测的Y值。</p><p>Now, we only need to determine the values for $\beta_1$ and $\beta_0$ such that $h(x_i)$ is close to $Y_i$ as much as possible.</p><p>现在，我们需要确定$\beta_1$和$\beta_0$的值，使得我们预测出的每一个 $h(x_i)$都尽可能的接近真实值$y_i$.</p><p>Therefore, we need to create a indicator to quantify how close the predicted response is close to actual response. The mean squared error loss function is used here as the indicator:</p><p>因此我们需要建立一个量化指标来衡量到底预测值$h(x_i)$与真实值$y_i$有多接近。我们将使用平方和损失函数（目标函数）作为这个量化指标：</p><p>$L(\beta_0, \beta_1) = \frac{1}{N}\sum_{i=1}^N(\beta_0+\beta_1 x_i - y_i)^2$</p><p>Based on this loss function, we try to find optimized values for $\beta_0$ and $\beta_1$ such that the loss is minimized.</p><p>借助于这个损失函数，我们试图寻找$\beta_0$和$\beta_1$的最优解，在这个最优解下，损失函数的值最小。</p><p>If this problem is solved manually, we can just take the derivative of the loss function with respect to $\beta_1$ or $\beta_0$. In this case, we can obtain the optimized values directly.</p><p>如果我们试图用手算来解决这个问题，我们可以直接将损失函数对$\beta_0$和$\beta_1$求导数，即可获得$\beta_0$和$\beta_1$的解。</p><p>Here, we would like to solve this problem using computer programming. Therefore, optimization algorith gradient descent is involved.</p><p>这里，我们想写计算机代码来解决这个问题，因此我们要用到叫做“梯度下降”的优化算法。</p><p>Graident descent will be introduced later in this session.</p><p>梯度下降的算法将会在稍后进行介绍。</p><p>The plot for the loss function is presented below (python code is also provided):</p><p>平方和损失函数如下图所示（python源代码一并提供）：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Import the package for three-dimensional plot</span></span><br><span class="line"><span class="keyword">from</span> mpl_toolkits.mplot3d <span class="keyword">import</span> Axes3D</span><br><span class="line"></span><br><span class="line"><span class="comment"># Generate values for beta_0 and beta_1</span></span><br><span class="line">number_beta = <span class="number">100</span></span><br><span class="line">beta_0 = np.linspace(-<span class="number">5</span>, <span class="number">10</span>, number_beta)</span><br><span class="line">beta_1 = np.linspace(-<span class="number">1</span>, <span class="number">5</span>, number_beta)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Create a grid to evaluate loss function values on the grid</span></span><br><span class="line"><span class="comment"># You can either use command in line #11 or comand in line #18, 19, 23, 24 to create grid</span></span><br><span class="line"><span class="comment"># grid_beta_0, grid_beta_1 = np.meshgrid(beta_0, beta_1)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Calculate the loss function values under different beta_0 and beta_1</span></span><br><span class="line"><span class="comment"># Initialize a matrix to store loss function values</span></span><br><span class="line">loss = np.zeros([number_beta, number_beta])</span><br><span class="line"></span><br><span class="line"><span class="comment"># Initialize a matrix to store grid values</span></span><br><span class="line">grid_beta_0 = np.zeros([number_beta, number_beta])</span><br><span class="line">grid_beta_1 = np.zeros([number_beta, number_beta])</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(number_beta):</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(number_beta):</span><br><span class="line">        grid_beta_0[i,j] = beta_0[i]</span><br><span class="line">        grid_beta_1[i,j] = beta_1[j]</span><br><span class="line">        predict_response = beta_0[i] + beta_1[j] * X</span><br><span class="line">        loss[i,j] = np.dot((predict_response - Y).T, (predict_response-Y))/number_observation</span><br><span class="line"></span><br><span class="line"><span class="comment"># Plot the loss function values over the beta_0 and beta_1 values</span></span><br><span class="line">fig = plt.figure(figsize=(<span class="number">16</span>,<span class="number">8</span>))</span><br><span class="line">ax = Axes3D(fig)</span><br><span class="line">ax.set_xlabel(<span class="string">r&quot;$\beta_0$&quot;</span>, font)</span><br><span class="line">ax.set_ylabel(<span class="string">r&quot;$\beta_1$&quot;</span>, font)</span><br><span class="line">ax.set_zlabel(<span class="string">&quot;Loss function value&quot;</span>, font)</span><br><span class="line">plt.xticks(fontsize=<span class="number">16</span>, fontname=<span class="string">&#x27;Times New Roman&#x27;</span>)</span><br><span class="line">plt.yticks(fontsize=<span class="number">16</span>, fontname=<span class="string">&#x27;Times New Roman&#x27;</span>)</span><br><span class="line">ax.plot_surface(grid_beta_0, grid_beta_1, loss, cmap=<span class="string">&#x27;rainbow&#x27;</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="https://raw.githubusercontent.com/GUAN-XINGQUAN/images/main/img/20210308231952.png"></p><p>The figure above shows that a global minimum exists for this loss function. This global minimum is the case that we want to achieve because the loss is smallest. In other words, the predicted response is closest to the actual response.</p><p>上图展示了我们损失函数有一个全局最小值，而这个最小值正是我们试图寻找的。该最小值表征了我们预测的Y值与真实Y值是最接近的。</p><p>As mentioned above, we are using gradient descent to find the global minimum. For those who are not familiar with gradient descent, please refer to the following link for more information:</p><p><a href="https://en.wikipedia.org/wiki/Gradient_descent">https://en.wikipedia.org/wiki/Gradient_descent</a></p><p>如上文提到的，我们将使用梯度下降方法来寻找全局最小值。对于还不是很熟悉梯度下降概念的同学，可以点击下面链接来了解梯度下降方法：</p><p><a href="https://baike.baidu.com/item/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D/4864937?fr=aladdin">https://baike.baidu.com/item/梯度下降/4864937?fr=aladdin</a></p><p>For gradient descent algorithm, the first step is to give arbitary values for $\beta_0$ and $\beta_1$.</p><p>梯度下降方法中，我们先随机赋予$\beta_0$和$\beta_1$两个数值。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># These two values are given arbitrarily</span></span><br><span class="line"><span class="comment"># You can give any values as you like</span></span><br><span class="line">beta_0 = <span class="number">1</span></span><br><span class="line">beta_1 = <span class="number">0.5</span></span><br><span class="line">predict_response = beta_0 + beta_1 * X</span><br><span class="line"></span><br><span class="line"><span class="comment"># Visualize the fitted line</span></span><br><span class="line">plt.figure(figsize=(<span class="number">16</span>,<span class="number">8</span>))</span><br><span class="line">plt.scatter(X, Y, color=<span class="string">&#x27;r&#x27;</span>, label=<span class="string">&#x27;original data points&#x27;</span>)</span><br><span class="line">plt.plot(X, predict_response, color=<span class="string">&#x27;g&#x27;</span>, linewidth=<span class="number">2.0</span>, label=<span class="string">&#x27;fitted line&#x27;</span>)</span><br><span class="line">font = &#123;<span class="string">&#x27;family&#x27;</span>:<span class="string">&#x27;Times New Roman&#x27;</span>,<span class="string">&#x27;weight&#x27;</span> : <span class="string">&#x27;normal&#x27;</span>,<span class="string">&#x27;size&#x27;</span>: <span class="number">16</span>,&#125;</span><br><span class="line">plt.xlabel(<span class="string">&#x27;X&#x27;</span>, font)</span><br><span class="line">plt.ylabel(<span class="string">&#x27;Y&#x27;</span>, font)</span><br><span class="line">plt.xticks(fontsize=<span class="number">16</span>, fontname=<span class="string">&#x27;Times New Roman&#x27;</span>)</span><br><span class="line">plt.yticks(fontsize=<span class="number">16</span>, fontname=<span class="string">&#x27;Times New Roman&#x27;</span>)</span><br><span class="line">plt.title(<span class="string">&#x27;Original data points and fitted line&#x27;</span>, font)</span><br><span class="line">plt.legend(fontsize=<span class="number">16</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="https://raw.githubusercontent.com/GUAN-XINGQUAN/images/main/img/20210308232044.png"></p><p>As shown in the figure above, the initial fitted line does not match the data points. This is not surprised as the values of $\beta_0$ and $\beta_1$ are given arbritarily.</p><p>上图展示了拟合的直线与离散的数据点吻合并不是很好。这并不出乎意料，因为我们只是随意的给$\beta_0$和$\beta_1$赋值。</p><p>Don’t worry, we will use gradient descent to “train” $\beta_0$ and $\beta_1$. Eventually, a good fitted line will be obtained.</p><p>我们会用梯度下降来“训练”$\beta_0$和$\beta_1$，使得最终获得的较好的拟合效果。</p><p>We need to find the gradient of the loss function, i.e., the derivative with respect to $\beta_0$ and $\beta_1$:</p><p>首先，我们需要寻找函数的“梯度”，即关于$\beta_0$和$\beta_1$的导数：</p><p>$\frac{\partial L(\beta_0,\beta_1)}{\partial \beta_0} = \frac{2}{N}\sum_{i=1}^{N}(\beta_0 + \beta_1 x_i - y_i)$</p><p>$\frac{\partial L(\beta_0,\beta_1)}{\partial \beta_1} = \frac{2}{N}\sum_{i=1}^{N}(\beta_0 + \beta_1 x_i - y_i)x_i$</p><p>Then we just need to use the negative gradient direction to update $\beta_0$ and $beta_1$</p><p>然后，我们沿着梯度的负方向来迭代更新$\beta_0$和$\beta_1$两个参数。</p><p>$\beta_0^{n+1} = \beta_0^n - s\frac{\partial L(\beta_0^n, \beta_1^n)}{\partial \beta_0^n}$</p><p>$\beta_1^{n+1} = \beta_1^n - s\frac{\partial L(\beta_0^n, \beta_1^n)}{\partial \beta_1^n}$</p><p>where s is the step size. Step size cannot be neither too large nor too small. If it is too large, then we may miss the global minimum value. If it is too small, this update algorithm would be time-consuming.</p><p>表达式中的s是代表步长。步长不能太长，也不能太短。步长如果太长，会使得我们错过了全局最小值。如果步长太短，会使得算法花很长的时间来收敛。</p><p>Then the question is how to determine the value for step size. Honestly, I don’t know the answer. Evertime I did this. I always given an arbitrary value for the step size. If it doesn’t work well, then I just change the value for step size, and try again. It’s like a trial and error process.</p><p>那么如何确定步长的大小。这是一个“玄学”。至少我本人不是很了解如何确定。我通常的做法是给定一个步长，跑整个程序。如果结果不好，我再修改步长，再运行，直到结果比较满意为止。</p><p>The following section gives the code on how we perform gradience descent algorithm. The final results are also shown below.</p><p>接下来的代码展示了我们如何使用梯度下降方法。最终结果也一并画图呈现。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Assume we are going to iterate 5000 times</span></span><br><span class="line">max_iteration = <span class="number">5000</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Assume step size is 0.001</span></span><br><span class="line">step_size = <span class="number">0.001</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Initialize beta_0, beta_1, and loss to track the update algorithm</span></span><br><span class="line">beta_0_update = np.zeros([max_iteration+<span class="number">1</span>, <span class="number">1</span>])</span><br><span class="line">beta_1_update = np.zeros([max_iteration+<span class="number">1</span>, <span class="number">1</span>])</span><br><span class="line">loss_update = np.zeros([max_iteration+<span class="number">1</span>, <span class="number">1</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># Initial value for beta_0 and beta_1</span></span><br><span class="line">beta_0_update[<span class="number">0</span>,<span class="number">0</span>] = beta_0</span><br><span class="line">beta_1_update[<span class="number">0</span>,<span class="number">0</span>] = beta_1</span><br><span class="line"></span><br><span class="line"><span class="comment"># Iterate many times in order to get the best values for beta_0 and beta_1</span></span><br><span class="line"><span class="keyword">for</span> <span class="built_in">iter</span> <span class="keyword">in</span> <span class="built_in">range</span>(max_iteration):</span><br><span class="line">    predict_response = beta_0_update[<span class="built_in">iter</span>,<span class="number">0</span>] + beta_1_update[<span class="built_in">iter</span>,<span class="number">0</span>] * X</span><br><span class="line">    difference = predict_response - Y</span><br><span class="line">    loss_update[<span class="built_in">iter</span>,<span class="number">0</span>] = np.dot(difference.T, difference)/number_observation</span><br><span class="line">    gradient_beta_0 = <span class="number">2</span> * np.<span class="built_in">sum</span>(difference)/number_observation</span><br><span class="line">    gradient_beta_1 = <span class="number">2</span> * np.dot(difference, X)/number_observation</span><br><span class="line">    beta_0_update[<span class="built_in">iter</span>+<span class="number">1</span>] = beta_0_update[<span class="built_in">iter</span>] - step_size * gradient_beta_0</span><br><span class="line">    beta_1_update[<span class="built_in">iter</span>+<span class="number">1</span>] = beta_1_update[<span class="built_in">iter</span>] - step_size * gradient_beta_1</span><br><span class="line"></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">Y_predict = beta_0_update[-<span class="number">1</span>,<span class="number">0</span>]+ beta_1_update[-<span class="number">1</span>,<span class="number">0</span>] * X</span><br><span class="line"></span><br><span class="line">plt.figure(figsize=(<span class="number">16</span>,<span class="number">8</span>))</span><br><span class="line">plt.scatter(X, Y, color=<span class="string">&#x27;r&#x27;</span>, label=<span class="string">&#x27;original data points&#x27;</span>)</span><br><span class="line">plt.plot(X, Y_predict, color=<span class="string">&#x27;g&#x27;</span>, linewidth=<span class="number">2.0</span>, label=<span class="string">&#x27;fitted line&#x27;</span>)</span><br><span class="line">font = &#123;<span class="string">&#x27;family&#x27;</span>:<span class="string">&#x27;Times New Roman&#x27;</span>,<span class="string">&#x27;weight&#x27;</span> : <span class="string">&#x27;normal&#x27;</span>,<span class="string">&#x27;size&#x27;</span>: <span class="number">16</span>,&#125;</span><br><span class="line">plt.xlabel(<span class="string">&#x27;X&#x27;</span>, font)</span><br><span class="line">plt.ylabel(<span class="string">&#x27;Y&#x27;</span>, font)</span><br><span class="line">plt.xticks(fontsize=<span class="number">16</span>, fontname=<span class="string">&#x27;Times New Roman&#x27;</span>)</span><br><span class="line">plt.yticks(fontsize=<span class="number">16</span>, fontname=<span class="string">&#x27;Times New Roman&#x27;</span>)</span><br><span class="line">plt.title(<span class="string">&#x27;Original data points and final fitted line&#x27;</span>, font)</span><br><span class="line">plt.legend(fontsize=<span class="number">16</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="https://raw.githubusercontent.com/GUAN-XINGQUAN/images/main/img/20210308232110.png"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">plt.figure(figsize=(<span class="number">16</span>,<span class="number">8</span>))</span><br><span class="line">plt.plot(<span class="built_in">range</span>(max_iteration), loss_update[<span class="number">0</span>:max_iteration,<span class="number">0</span>], color=<span class="string">&#x27;g&#x27;</span>, linewidth=<span class="number">2.0</span>, label=<span class="string">&#x27;fitted line&#x27;</span>)</span><br><span class="line">font = &#123;<span class="string">&#x27;family&#x27;</span>:<span class="string">&#x27;Times New Roman&#x27;</span>,<span class="string">&#x27;weight&#x27;</span> : <span class="string">&#x27;normal&#x27;</span>,<span class="string">&#x27;size&#x27;</span>: <span class="number">16</span>,&#125;</span><br><span class="line">plt.xlabel(<span class="string">&#x27;iteration times&#x27;</span>, font)</span><br><span class="line">plt.ylabel(<span class="string">&#x27;loss function values&#x27;</span>, font)</span><br><span class="line">plt.xticks(fontsize=<span class="number">16</span>, fontname=<span class="string">&#x27;Times New Roman&#x27;</span>)</span><br><span class="line">plt.yticks(fontsize=<span class="number">16</span>, fontname=<span class="string">&#x27;Times New Roman&#x27;</span>)</span><br><span class="line">plt.title(<span class="string">&#x27;Loss function value vs. iteration times&#x27;</span>, font)</span><br><span class="line">plt.legend(fontsize=<span class="number">16</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="https://raw.githubusercontent.com/GUAN-XINGQUAN/images/main/img/20210308232133.png"></p><p>As observed from the plot of loss function value vs. iteration times, the loss already reaches convergence. This means that even we increase the iteration times, the loss would not further decrease. The final fitted line and data points are shown in the figure above.</p><p>从损失函数与迭代次数的关系图象可以看出，损失函数早已收敛。这意味着，即便我们增加迭代次数，损失函数的值不会进一步减小。最终拟合曲线与离散点在上文的图也已展示。</p><p>OK. We are done. Feel free to leave any comments.</p><p>到此结束。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;I have been learning the statistics for about one year. Nine months ago, I didn’t believe that I can survive from statistics classes as I</summary>
      
    
    
    
    <category term="Blog" scheme="http://example.com/categories/Blog/"/>
    
    
  </entry>
  
  <entry>
    <title>test_my_site</title>
    <link href="http://example.com/2021/03/06/test-my-site/"/>
    <id>http://example.com/2021/03/06/test-my-site/</id>
    <published>2021-03-07T06:32:22.000Z</published>
    <updated>2021-03-07T06:32:22.840Z</updated>
    
    
    
    
    
  </entry>
  
</feed>
