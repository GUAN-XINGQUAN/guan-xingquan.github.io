<!DOCTYPE html><html lang="en"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=5"><title>Guan's Website</title><meta name="author" content="Xingquan Guan"><link rel="shortcut icon" href="/img/favicon.png"><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.13.0/css/all.min.css"><meta name="generator" content="Hexo 5.4.2"><link rel="alternate" href="/atom.xml" title="Guan's Website" type="application/atom+xml">
</head><body><header id="page_header"><div class="header_wrap"><div id="blog_name"><a class="blog_title" id="site-name" href="/">Guan's Website</a></div><button class="menus_icon"><div class="navicon"></div></button><ul class="menus_items"><li class="menus_item"><a class="site-page" href="/About"> About</a></li><li class="menus_item"><a class="site-page" href="/Research"> Research</a></li><li class="menus_item"><a class="site-page" href="/Publications"> Publications</a></li><li class="menus_item"><a class="site-page" href="/Teaching"> Teaching</a></li><li class="menus_item"><a class="site-page" href="/Resources"> Resources</a></li><li class="menus_item"><a class="site-page" href="/Blog"> Blog</a></li></ul></div></header><main id="page_main"><div class="side-card sticky"><div class="card-wrap" itemscope itemtype="http://schema.org/Person"><div class="author-avatar"><img class="avatar-img" src="/img/profile.png" onerror="this.onerror=null;this.src='/img/profile.png'" alt="avatar"></div><div class="author-discrip"><h3>Xingquan Guan</h3><p class="author-bio">Data Scientist | Researcher | Hazard Engineering + Artificial Intelligence</p></div><div class="author-links"><button class="btn m-social-links">Links</button><ul class="social-icons"><li><a class="social-icon" href="https://github.com/GUAN-XINGQUAN/" target="_blank"><i class="fab fa-github" aria-hidden="true"></i></a></li><li><a class="social-icon" href="https://www.linkedin.com/in/guanxingquan/" target="_blank"><i class="fab fa-linkedin" aria-hidden="true"></i></a></li><li><a class="social-icon" href="tencent://AddContact/?fromId=50&amp;fromSubId=1&amp;subcmd=all&amp;uin=619460794/" target="_blank"><i class="fab fa-qq" aria-hidden="true"></i></a></li><li><a class="social-icon" href="mailto:guanxingquan@ucla.edu/" target="_blank"><i class="fas fa-envelope" aria-hidden="true"></i></a></li></ul><ul class="social-links"><li><a class="e-social-link" href="https://scholar.google.com/citations?user=KXiOdokAAAAJ&amp;hl=en&amp;authuser=1/ /" target="_blank"><i class="fas fa-graduation-cap" aria-hidden="true"></i><span>Google Scholar</span></a></li><li><a class="e-social-link" href="https://www.researchgate.net/profile/xingquan-guan/ /" target="_blank"><i class="fab fa-researchgate" aria-hidden="true"></i><span>Research Gate</span></a></li></ul></div><a class="cv-links" href="/attaches/XINGQUAN%20GUAN%20CV.pdf" target="_blank"><i class="fas fa-file-pdf" aria-hidden="true"><span>My CV.</span></i></a></div></div><div class="page" itemscope itemtype="http://schema.org/CreativeWork"><h2 class="page-title">Week 2 Logistic Regression</h2><article><p>————————————— start from here ——————————————-</p>
<p>Background (问题背景):</p>
<p>Let’s consider a binary classification. For example, students are going to apply for student loan from a financial institution. The instution decides whether it will approve the student application based on two factors: the student’s credit score and his/her monthly income. Now we can use $X1$ to represent the student’s monthly income and $X2$ to represent the student’s credit score. Then we can use $Y \in {1,0}$ to denote the application results. $Y_i=1$ means that the institution approves the student’s application and 0 for the reject of application.</p>
<p>Imagine that we have 100 students who will apply for the student loan. Obviously, some of them can get the load whereas others get rejected.</p>
<p>To model this problem, we can use logistic regression to classify the students who can and who cannot get the loan. $X1$ and $X2$ are two column vectors with the length of 100. Y is a column vector whose length is also 100. Each element in Y can either be 1 or 0.</p>
<p>我们现在来考虑一个二分类问题。例如，每年都会有学生向银行申请学生贷款。一般而言，银行会根据两个因素来判定是否通过该学生贷款的申请：信用记录分数以及该学生每个月的收入。这样的话，我们可以用$X1$和$X2$分别表达学生的月收入和信用分数。然后我们可以用$Y \in {1,0}$来表达该学生是否可以成功从银行取得贷款。Y=1表示该学生成功获得贷款，而Y=0表示该生贷款申请遭到拒绝。</p>
<p>假设我们有200个学生向银行申请贷款。显然，这200名学生中有人将取得贷款，而另一部分学生可能因为较差的信用分数或者较低的月收入而申请被拒。</p>
<p>我们可以利用逻辑回归对该问题建模。这里，$X1$和$X2$分别是一个长度为100的列向量。Y也是一个长度为200的列向量。Y中的每一个元素只可能是1或者0.</p>
<p>First of all, the data of X1, X2, and Y are generated using some random function.</p>
<p>首先我们来生成X1, X2, 和Y的数据。</p>
<p>(It should be noted that the generation for X1, X2, and Y are kind of arbitrary as we don’t have actual data from financial institution)</p>
<p>这里需要声明的是，X1, X2以及Y的是随意生成的，因为我们没有实际的数据。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Add this command because of plot</span></span><br><span class="line"><span class="comment"># Otherwise, the plot may not show up in Anaconda</span></span><br><span class="line">% matplotlib inline</span><br><span class="line"></span><br><span class="line"><span class="comment"># Import necessary packages</span></span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt  <span class="comment"># Package used for plotting the figure</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np  <span class="comment"># Package relating to math</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Define length of X1 vector, which is dentical to the length of X2 and Y</span></span><br><span class="line"><span class="comment"># In total, 200 students are applying for the student load</span></span><br><span class="line">number_observation = <span class="number">200</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Generate X1 and X2</span></span><br><span class="line">X1 = np.linspace(<span class="number">1600</span>, <span class="number">2500</span>, number_observation)  <span class="comment"># Assume the students&#x27; monthly income ranges from 1600 to 2500</span></span><br><span class="line">X2 = X1/<span class="number">2.5</span> + np.random.normal(<span class="number">0</span>, <span class="number">100</span>, number_observation)  <span class="comment"># Assume the credit scrore is following this function</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Generate Y vector</span></span><br><span class="line">Y = np.linspace(<span class="number">0</span>, <span class="number">0</span>, number_observation)</span><br><span class="line"><span class="comment"># indicator represents whther the financial institution approves the student&#x27;s application or not</span></span><br><span class="line"><span class="comment"># indicator &lt; 0 means the application is rejected.</span></span><br><span class="line"><span class="comment"># indicator &gt;= 0 means the applcation is approved.</span></span><br><span class="line"><span class="comment"># A noise term is added because I don&#x27;t want the data to be linearly saparable</span></span><br><span class="line">indicator = (X1 - np.mean(X1))/np.std(X1) + (X2 - np.mean(X2))/np.std(X2) + np.random.normal(<span class="number">0</span>, <span class="number">0.25</span>, number_observation)</span><br><span class="line">Y[indicator &lt; <span class="number">0</span>] = <span class="number">0</span></span><br><span class="line">Y[indicator &gt;= <span class="number">0</span>] = <span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Visualize the data points</span></span><br><span class="line">plt.figure(figsize=(<span class="number">16</span>,<span class="number">8</span>))</span><br><span class="line">plt.scatter(X1[indicator &lt; <span class="number">0</span>], X2[indicator &lt; <span class="number">0</span>], color=<span class="string">&#x27;b&#x27;</span>, marker=<span class="string">&#x27;o&#x27;</span>, label=<span class="string">&#x27;Rejected&#x27;</span>)</span><br><span class="line">plt.scatter(X1[indicator &gt;= <span class="number">0</span>], X2[indicator &gt;= <span class="number">0</span>], color=<span class="string">&#x27;r&#x27;</span>, marker=<span class="string">&#x27;*&#x27;</span> , label=<span class="string">&#x27;Approved&#x27;</span>)</span><br><span class="line">font = &#123;<span class="string">&#x27;family&#x27;</span>:<span class="string">&#x27;Times New Roman&#x27;</span>,<span class="string">&#x27;weight&#x27;</span> : <span class="string">&#x27;normal&#x27;</span>,<span class="string">&#x27;size&#x27;</span>: <span class="number">16</span>,&#125;</span><br><span class="line">plt.xlabel(<span class="string">&#x27;X1&#x27;</span>, font)</span><br><span class="line">plt.ylabel(<span class="string">&#x27;X2&#x27;</span>, font)</span><br><span class="line">plt.xticks(fontsize=<span class="number">16</span>, fontname=<span class="string">&#x27;Times New Roman&#x27;</span>)</span><br><span class="line">plt.yticks(fontsize=<span class="number">16</span>, fontname=<span class="string">&#x27;Times New Roman&#x27;</span>)</span><br><span class="line">plt.title(<span class="string">&#x27;The distibution of X1 and X2&#x27;</span>, font)</span><br><span class="line">plt.legend(fontsize=<span class="number">16</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>


<p><img src="https://raw.githubusercontent.com/GUAN-XINGQUAN/images/main/img/20210309101334.png"></p>
<p>Later we will use sigmoid function to deal with these data points. As we know, sigmoid function invovles the exponential algorithm. Since X1 is large (the maximum is about 2400), we may encounter overflow problem in coding. To avoid the overflow problem, we have to normailize X1 and X2.</p>
<p>在后面我们将会用sigmoid函数。这个函数包含了以自然指数e为底的运算。因为X1的数值很大，因此我们很有可能会碰到数值溢出的问题。为了避免该类问题，我们需要将X1和X2正则化。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Normalize X1 and X2</span></span><br><span class="line">new_X1 = (X1 - np.mean(X1))/np.std(X1)</span><br><span class="line">new_X2 = (X2 - np.mean(X2))/np.std(X2)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Visualize the data points</span></span><br><span class="line">plt.figure(figsize=(<span class="number">16</span>,<span class="number">8</span>))</span><br><span class="line">plt.scatter(new_X1[indicator &lt; <span class="number">0</span>], new_X2[indicator &lt; <span class="number">0</span>], color=<span class="string">&#x27;b&#x27;</span>, marker=<span class="string">&#x27;o&#x27;</span>, label=<span class="string">&#x27;Rejected&#x27;</span>)</span><br><span class="line">plt.scatter(new_X1[indicator &gt;= <span class="number">0</span>], new_X2[indicator &gt;= <span class="number">0</span>], color=<span class="string">&#x27;r&#x27;</span>, marker=<span class="string">&#x27;*&#x27;</span> , label=<span class="string">&#x27;Approved&#x27;</span>)</span><br><span class="line">font = &#123;<span class="string">&#x27;family&#x27;</span>:<span class="string">&#x27;Times New Roman&#x27;</span>,<span class="string">&#x27;weight&#x27;</span> : <span class="string">&#x27;normal&#x27;</span>,<span class="string">&#x27;size&#x27;</span>: <span class="number">16</span>,&#125;</span><br><span class="line">plt.xlabel(<span class="string">&#x27;new X1&#x27;</span>, font)</span><br><span class="line">plt.ylabel(<span class="string">&#x27;new X2&#x27;</span>, font)</span><br><span class="line">plt.xticks(fontsize=<span class="number">16</span>, fontname=<span class="string">&#x27;Times New Roman&#x27;</span>)</span><br><span class="line">plt.yticks(fontsize=<span class="number">16</span>, fontname=<span class="string">&#x27;Times New Roman&#x27;</span>)</span><br><span class="line">plt.title(<span class="string">&#x27;The distibution of new X1 and X2&#x27;</span>, font)</span><br><span class="line">plt.legend(fontsize=<span class="number">16</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>


<p><img src="https://raw.githubusercontent.com/GUAN-XINGQUAN/images/main/img/20210309101349.png"></p>
<p>As we can see from the figure, the process of normalization only affects the values of X1 and X2. It doesn’t affect the distribution of X1 and X2 (the layout of X1 and X2 are still the same).</p>
<p>从上图可以看出，对X1和X2正则化只是影响了横纵坐标的数值大小，并不会影响数据的分布趋势。</p>
<p>In the figure, red stars represent the case that a student’s loan application is approved, whereas the blue dots represent the case that a student’s loan application is declined.</p>
<p>图中，红色五角星表示学生的贷款申请被批准；蓝色圆心表示学生的贷款申请被拒。</p>
<p>Although the data points are generated in an arbitrary method, these data points are consistent with our common sense. A student with higher monthly income and higher credit score tends to get approved for his appliation. In contrast, a student with low income and low credit score tends to be rejected for applying the loan.</p>
<p>尽管这些数据点是随意生成的，但是其符合我们的正常认知。当一个学生有较高的月收入和较高的信用分数，他将会大概率获得贷款。而当一个学生的月收入较低，信用分数也很低时，他的贷款申请将会被拒。</p>
<p>Now, we would like to use the sigmoid function to quantify the possibility (or probability) for a single student to get the loan.</p>
<p>现在我们需要用下面的这个叫做sigmoid的函数来表征单个学生能够拿到贷款的概率:</p>
<p>$h^i(X_1, X_2) = \frac{e^{\beta_1 X_1^i + \beta_2 X_2^i}}{1 + e^{\beta_1 X_1^i + \beta_2 X_2^i}}$</p>
<p>where the superscript $i$ represents the $i^{th}$ student.</p>
<p>式中上标$i$表示第$i$个学生。</p>
<p>The question is then how to determine the two parameters: $\beta_1$ and $\beta_2$</p>
<p>现在的问题便是如何确定$\beta_1$和$\beta_2$的数值。</p>
<p>As what we did in week 1 Linear regression, we need to introduce an objective function (loss function). Then we use gradient descent to find the global minimum of the objective function.</p>
<p>正如我们在第一周线性回归中讲述的一样，我们需要引入一个目标函数（损失函数）。然后我们利用梯度下降方法来寻找目标函数的最小值。</p>
<p>The objective function is listed below and it is revised from the maximum likelihood function of logistic regression:</p>
<p>目标函数定义如下，它是基于逻辑回归的最大似然函数修改得到的：</p>
<p>$L(\beta_1, \beta_2) = -\frac{1}{N}\sum_{i=0}^N{y^i ln[h^i(X_1^i, X_2^i)]+(1-y^i) ln[1-h(X_1^i, X_2^i)]}$</p>
<p>The plot for the objective function is shown below:</p>
<p>该目标函数图象如下所示：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Import the package for three-dimensional plot</span></span><br><span class="line"><span class="keyword">from</span> mpl_toolkits.mplot3d <span class="keyword">import</span> Axes3D</span><br><span class="line"></span><br><span class="line"><span class="comment"># Generate values for beta_1 and beta_2</span></span><br><span class="line">number_beta = <span class="number">100</span></span><br><span class="line">beta_1 = np.linspace(<span class="number">2</span>, <span class="number">8</span>, number_beta)</span><br><span class="line">beta_2 = np.linspace(<span class="number">1</span>, <span class="number">8</span>, number_beta)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Initialize a matrix to store loss function values</span></span><br><span class="line">loss = np.zeros([number_beta, number_beta])</span><br><span class="line"></span><br><span class="line"><span class="comment"># Initialize a matrix to store grid values</span></span><br><span class="line">grid_beta_1 = np.zeros([number_beta, number_beta])</span><br><span class="line">grid_beta_2 = np.zeros([number_beta, number_beta])</span><br><span class="line"></span><br><span class="line"><span class="comment"># Calculate the loss function values under different beta_1 and beta_2</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(number_beta):</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(number_beta):</span><br><span class="line">        grid_beta_1[i,j] = beta_1[i]</span><br><span class="line">        grid_beta_2[i,j] = beta_2[j]</span><br><span class="line">        s = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> indx <span class="keyword">in</span> <span class="built_in">range</span>(number_observation):</span><br><span class="line">            h = np.exp(new_X1[indx] * beta_1[i] + new_X2[indx] * beta_2[j]) / (<span class="number">1</span>+np.exp(new_X1[indx] * beta_1[i] + new_X2[indx] * beta_2[j]))</span><br><span class="line">            temp = Y[indx]*np.log(h) + (<span class="number">1</span>-Y[indx])*np.log(<span class="number">1</span>-h)</span><br><span class="line">            s = s + temp</span><br><span class="line">        loss[i,j] = -<span class="number">1</span>/number_observation * s</span><br><span class="line">        </span><br><span class="line"><span class="comment"># Plot the loss function values over the beta_0 and beta_1 values</span></span><br><span class="line">fig = plt.figure(figsize=(<span class="number">16</span>,<span class="number">8</span>))</span><br><span class="line">ax = Axes3D(fig)</span><br><span class="line">ax.set_xlabel(<span class="string">r&quot;$\beta_1$&quot;</span>, font)</span><br><span class="line">ax.set_ylabel(<span class="string">r&quot;$\beta_2$&quot;</span>, font)</span><br><span class="line">ax.set_zlabel(<span class="string">&quot;Loss function value&quot;</span>, font)</span><br><span class="line">plt.xticks(fontsize=<span class="number">16</span>, fontname=<span class="string">&#x27;Times New Roman&#x27;</span>)</span><br><span class="line">plt.yticks(fontsize=<span class="number">16</span>, fontname=<span class="string">&#x27;Times New Roman&#x27;</span>)</span><br><span class="line">ax.plot_surface(grid_beta_1, grid_beta_2, loss, cmap=<span class="string">&#x27;rainbow&#x27;</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>


<p><img src="https://raw.githubusercontent.com/GUAN-XINGQUAN/images/main/img/20210309101419.png"></p>
<p>From the figure above, the objective function has a global minimum and we are trying to find this global minimum. Again we will use gradient descent to reach our target.</p>
<p>从上图可以看出，目标函数是有全局最小值的。我们会用梯度下降的方法来寻找该最小值。</p>
<p>Let’s calculate the derivate for the objective function.</p>
<p>先来计算目标函数关于$\beta_1$和$\beta_2$的导数。</p>
<p>$\frac{\partial L(\beta_1,\beta_2)}{\partial \beta_1}= -\frac{1}{N}\sum_{i=1}^N {y^i \frac{1}{h^i} (h^i)’ + (1-y^i \frac{1}{1-h^i} (-h^i)’}$</p>
<p>$=-\frac{1}{N}\sum_{i=1}^N {\frac{y^i}{h^i} - \frac{(1-y^i)}{1-h^i}} (h^i)’$</p>
<p>$=-\frac{1}{N}\sum_{i=1}^N {\frac{y^i}{h^i} - \frac{(1-y^i)}{1-h^i}} h^i (1-h^i) x_1^i$</p>
<p>$=-\frac{1}{N}\sum_{i=1}^N {y^i - h^i}x_1^i$</p>
<br>


<p>$\frac{\partial L(\beta_1,\beta_2)}{\partial \beta_2}= -\frac{1}{N}\sum_{i=1}^N {y^i \frac{1}{h^i} (h^i)’ + (1-y^i \frac{1}{1-h^i} (-h^i)’}$</p>
<p>$=-\frac{1}{N}\sum_{i=1}^N {\frac{y^i}{h^i} - \frac{(1-y^i)}{1-h^i}} (h^i)’$</p>
<p>$=-\frac{1}{N}\sum_{i=1}^N {\frac{y^i}{h^i} - \frac{(1-y^i)}{1-h^i}} h^i (1-h^i) x_2^i$</p>
<p>$=-\frac{1}{N}\sum_{i=1}^N {y^i - h^i}x_2^i$</p>
<p>Where the superscript $i$ means the $i^{th}$ element in X1, X2, or Y. $h$ is just shorthand for $h(X1, X2)$</p>
<p>上式中，上标$i$表示X1，X2或者Y中的第$i$个元素。$h$是$h(X1,X2)$的简写。</p>
<p>Then we can use the negative gradient direction to update the two parameters:</p>
<p>接下来我们可以用负梯度方向来更新两个参数：</p>
<p>$\beta_1^{n+1} = \beta_1^n - s\frac{\partial L(\beta_1^n, \beta_2^n)}{\partial \beta_1^n}$</p>
<p>$\beta_2^{n+1} = \beta_2^n - s\frac{\partial L(\beta_1^n, \beta_2^n)}{\partial \beta_2^n}$</p>
<p>First of all, let’s arbitrarily give two initial values to $\beta_1$ and $\beta_2$ and visualize the classification boundary.</p>
<p>首先我们先给$\beta_1$和$\beta_2$任意赋两个初始值，然后画出区分边界。</p>
<p>The boundary line is drawn using the following equation:</p>
<p>边界的直线由下面的等式来确定：</p>
<p>$\beta_1 xx + \beta_2 yy = 0$ </p>
<p>It should be noted that xx and yy here only refers to the coordinates of horizontal or vertical axes. They don’t refer to any predictor or response mentioned in the backgroun section. They are used for plotting the boundary only. No special meanings! Don’t get confused! $\beta_1$ and $\beta_2$ are two parameters of our interest, both of which are obtained from gradient descent algorithm.</p>
<p>需要注意的是，上面等式里面的xx和yy仅仅指二维坐标系中的坐标值，仅作画边界直线用，与问题背景交代中的x和y没有任何关系。不要混淆了。$\beta_1$和$\beta_2$是我们所感兴趣的参数，由梯度下降得到。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Two arbitrary values for beta_1 and beta_2</span></span><br><span class="line">beta_1 = <span class="number">0.1</span></span><br><span class="line">beta_2 = <span class="number">0.6</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Get the boundary</span></span><br><span class="line">xx = np.linspace(-<span class="number">2</span>, <span class="number">2</span>, <span class="number">100</span>)</span><br><span class="line">yy = -beta_1 * xx / beta_2</span><br><span class="line"></span><br><span class="line"><span class="comment"># Plot the data points and boundary line at initial beta_0 and beta_1</span></span><br><span class="line">fig = plt.figure(figsize=(<span class="number">16</span>,<span class="number">8</span>))</span><br><span class="line">plt.scatter(new_X1[indicator &lt; <span class="number">0</span>], new_X2[indicator &lt; <span class="number">0</span>], color=<span class="string">&#x27;b&#x27;</span>, marker=<span class="string">&#x27;o&#x27;</span>, label=<span class="string">&#x27;Rejected&#x27;</span>)</span><br><span class="line">plt.scatter(new_X1[indicator &gt;= <span class="number">0</span>], new_X2[indicator &gt;= <span class="number">0</span>], color=<span class="string">&#x27;r&#x27;</span>, marker=<span class="string">&#x27;*&#x27;</span> , label=<span class="string">&#x27;Approved&#x27;</span>)</span><br><span class="line">plt.plot(xx, yy, label=<span class="string">&#x27;Boundary&#x27;</span>)</span><br><span class="line">font = &#123;<span class="string">&#x27;family&#x27;</span>:<span class="string">&#x27;Times New Roman&#x27;</span>,<span class="string">&#x27;weight&#x27;</span> : <span class="string">&#x27;normal&#x27;</span>,<span class="string">&#x27;size&#x27;</span>: <span class="number">16</span>,&#125;</span><br><span class="line">plt.xlabel(<span class="string">&#x27;new X1&#x27;</span>, font)</span><br><span class="line">plt.ylabel(<span class="string">&#x27;new X2&#x27;</span>, font)</span><br><span class="line">plt.xticks(fontsize=<span class="number">16</span>, fontname=<span class="string">&#x27;Times New Roman&#x27;</span>)</span><br><span class="line">plt.yticks(fontsize=<span class="number">16</span>, fontname=<span class="string">&#x27;Times New Roman&#x27;</span>)</span><br><span class="line">plt.title(<span class="string">&#x27;The distibution of new X1 and X2&#x27;</span>, font)</span><br><span class="line">plt.legend(fontsize=<span class="number">16</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>


<p><img src="https://raw.githubusercontent.com/GUAN-XINGQUAN/images/main/img/20210309101444.png"></p>
<p>Not surprisingly, the line does not classify two types of data points well.</p>
<p>随意给出的这条直线并不能很好的划分两类数据点。</p>
<p>Now let’s use gradient descent to update $\beta_1$ and $\beta_2$.</p>
<p>现在用梯度下降的方法来更新$\beta_1$和$\beta_2$的数值：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Assume we are going to iterate 1000 times</span></span><br><span class="line">max_iteration = <span class="number">3000</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Define step size</span></span><br><span class="line">step_size = <span class="number">0.0001</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Initialize beta_1, beta_2, and loss to track the update algorithm</span></span><br><span class="line">beta_1_update = np.zeros([max_iteration+<span class="number">1</span>, <span class="number">1</span>])</span><br><span class="line">beta_2_update = np.zeros([max_iteration+<span class="number">1</span>, <span class="number">1</span>])</span><br><span class="line">loss_update = np.zeros([max_iteration+<span class="number">1</span>, <span class="number">1</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># Initial value for beta_1 and beta_2</span></span><br><span class="line">beta_1_update[<span class="number">0</span>,<span class="number">0</span>] = beta_1</span><br><span class="line">beta_2_update[<span class="number">0</span>,<span class="number">0</span>] = beta_2</span><br><span class="line"></span><br><span class="line"><span class="comment"># Iterate many times in order to get the best values for beta_0 and beta_1</span></span><br><span class="line"><span class="keyword">for</span> <span class="built_in">iter</span> <span class="keyword">in</span> <span class="built_in">range</span>(max_iteration):</span><br><span class="line">    <span class="comment"># Calculate h function results</span></span><br><span class="line">    h = np.exp(new_X1 * beta_1_update[<span class="built_in">iter</span>,<span class="number">0</span>] + new_X2 * beta_2_update[<span class="built_in">iter</span>,<span class="number">0</span>]) / (<span class="number">1</span> + np.exp(new_X1 * beta_1_update[<span class="built_in">iter</span>,<span class="number">0</span>] + new_X2 * beta_2_update[<span class="built_in">iter</span>,<span class="number">0</span>]))</span><br><span class="line">    <span class="comment"># Calculate the loss function value</span></span><br><span class="line">    loss_update[<span class="built_in">iter</span>,<span class="number">0</span>] = -np.mean(Y * np.log(h) + (<span class="number">1</span>-Y) * np.log(<span class="number">1</span>-h))</span><br><span class="line">    <span class="comment"># Calculate the gradient</span></span><br><span class="line">    gradient_beta_1 = -np.<span class="built_in">sum</span>((Y-h) * X1)/number_observation</span><br><span class="line">    gradient_beta_2 = -np.<span class="built_in">sum</span>((Y-h) * X2)/number_observation</span><br><span class="line">    <span class="comment"># Update beta_1 and beta_2</span></span><br><span class="line">    beta_1_update[<span class="built_in">iter</span>+<span class="number">1</span>] = beta_1_update[<span class="built_in">iter</span>] - step_size * gradient_beta_1</span><br><span class="line">    beta_2_update[<span class="built_in">iter</span>+<span class="number">1</span>] = beta_2_update[<span class="built_in">iter</span>] - step_size * gradient_beta_2</span><br><span class="line"></span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Plot the loss vs. iteration times</span></span><br><span class="line">plt.figure(figsize=(<span class="number">16</span>,<span class="number">8</span>))</span><br><span class="line">plt.plot(<span class="built_in">range</span>(max_iteration), loss_update[<span class="number">0</span>:max_iteration,<span class="number">0</span>], color=<span class="string">&#x27;g&#x27;</span>, linewidth=<span class="number">2.0</span>, label=<span class="string">&#x27;fitted line&#x27;</span>)</span><br><span class="line">font = &#123;<span class="string">&#x27;family&#x27;</span>:<span class="string">&#x27;Times New Roman&#x27;</span>,<span class="string">&#x27;weight&#x27;</span> : <span class="string">&#x27;normal&#x27;</span>,<span class="string">&#x27;size&#x27;</span>: <span class="number">16</span>,&#125;</span><br><span class="line">plt.xlabel(<span class="string">&#x27;iteration times&#x27;</span>, font)</span><br><span class="line">plt.ylabel(<span class="string">&#x27;loss function values&#x27;</span>, font)</span><br><span class="line">plt.xticks(fontsize=<span class="number">16</span>, fontname=<span class="string">&#x27;Times New Roman&#x27;</span>)</span><br><span class="line">plt.yticks(fontsize=<span class="number">16</span>, fontname=<span class="string">&#x27;Times New Roman&#x27;</span>)</span><br><span class="line">plt.title(<span class="string">&#x27;Loss function value vs. iteration times&#x27;</span>, font)</span><br><span class="line">plt.legend(fontsize=<span class="number">16</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>


<p><img src="https://raw.githubusercontent.com/GUAN-XINGQUAN/images/main/img/20210309101500.png"></p>
<p>As shown in the figure above, the loss already reaches convergence, which means increasing the iteration times would not further decrease the loss.</p>
<p>从上图可以看出，损失值已达到收敛。这意味着进一步增加迭代次数不会减少损失值。</p>
<p>The final boundary and data points are shown below:</p>
<p>最终的边界如下图所示：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Plot the data points and boundary line at final beta_0 and beta_1</span></span><br><span class="line">xx = np.linspace(-<span class="number">2</span>, <span class="number">2</span>, <span class="number">100</span>)</span><br><span class="line">yy = -beta_1_update[-<span class="number">1</span>,<span class="number">0</span>] * xx / beta_2_update[-<span class="number">1</span>,<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">fig = plt.figure(figsize=(<span class="number">16</span>,<span class="number">8</span>))</span><br><span class="line">plt.scatter(new_X1[indicator &lt; <span class="number">0</span>], new_X2[indicator &lt; <span class="number">0</span>], color=<span class="string">&#x27;b&#x27;</span>, marker=<span class="string">&#x27;o&#x27;</span>, label=<span class="string">&#x27;Rejected&#x27;</span>)</span><br><span class="line">plt.scatter(new_X1[indicator &gt;= <span class="number">0</span>], new_X2[indicator &gt;= <span class="number">0</span>], color=<span class="string">&#x27;r&#x27;</span>, marker=<span class="string">&#x27;*&#x27;</span> , label=<span class="string">&#x27;Approved&#x27;</span>)</span><br><span class="line">plt.plot(xx, yy, label=<span class="string">&#x27;Final boundary&#x27;</span>)</span><br><span class="line">font = &#123;<span class="string">&#x27;family&#x27;</span>:<span class="string">&#x27;Times New Roman&#x27;</span>,<span class="string">&#x27;weight&#x27;</span> : <span class="string">&#x27;normal&#x27;</span>,<span class="string">&#x27;size&#x27;</span>: <span class="number">16</span>,&#125;</span><br><span class="line">plt.xlabel(<span class="string">&#x27;new X1&#x27;</span>, font)</span><br><span class="line">plt.ylabel(<span class="string">&#x27;new X2&#x27;</span>, font)</span><br><span class="line">plt.xticks(fontsize=<span class="number">16</span>, fontname=<span class="string">&#x27;Times New Roman&#x27;</span>)</span><br><span class="line">plt.yticks(fontsize=<span class="number">16</span>, fontname=<span class="string">&#x27;Times New Roman&#x27;</span>)</span><br><span class="line">plt.title(<span class="string">&#x27;The distibution of new X1 and X2&#x27;</span>, font)</span><br><span class="line">plt.legend(fontsize=<span class="number">16</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>


<p><img src="https://raw.githubusercontent.com/GUAN-XINGQUAN/images/main/img/20210309101520.png"></p>
<p>As shwon in the figure above, the boundary separates the blue dots and red stars. Although there are some mis-classification points, these mis-classifications are inevitable as the original data points are not linear separable.</p>
<p>如上图所示，最终的边界直线可以较好的区分蓝色圆点和红色星状点。尽管有一些数据点没有被正确的划分，这种“错误”是无法避免的，因为我们原始的数据并不是被线性函数完全分割的(即两类数据点相互侵入对方区域)。</p>
<p>———————————– end —————————————————-</p>
<p>Review (往期回顾):</p>
<p>Week 1 Linear regression (线性回归)<br><a target="_blank" rel="noopener" href="https://blog.csdn.net/weixin_42515443/article/details/80768841">https://blog.csdn.net/weixin_42515443/article/details/80768841</a></p>
</article></div></main><div class="nav-wrap"><div class="nav"><button class="site-nav"><div class="navicon"></div></button><ul class="nav_items"><li class="nav_item"><a class="nav-page" href="/About"> About</a></li><li class="nav_item"><a class="nav-page" href="/Research"> Research</a></li><li class="nav_item"><a class="nav-page" href="/Publications"> Publications</a></li><li class="nav_item"><a class="nav-page" href="/Teaching"> Teaching</a></li><li class="nav_item"><a class="nav-page" href="/Resources"> Resources</a></li><li class="nav_item"><a class="nav-page" href="/Blog"> Blog</a></li></ul></div><div class="cd-top"><i class="fa fa-arrow-up" aria-hidden="true"></i></div></div><footer id="page_footer"><div class="footer_wrap"><div class="copyright">&copy;2020 - 2022 by Xingquan Guan</div><div class="theme-info">Powered by <a target="_blank" href="https://hexo.io" rel="nofollow noopener">Hexo</a> & <a target="_blank" href="https://github.com/PhosphorW/hexo-theme-academia" rel="nofollow noopener">Academia Theme</a></div></div></footer><script src="https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/jquery-pjax@latest/jquery.pjax.min.js"></script><script src="/js/main.js"></script></body></html>