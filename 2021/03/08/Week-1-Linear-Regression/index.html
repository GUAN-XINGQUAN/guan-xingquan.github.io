<!DOCTYPE html><html lang="en"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=5"><title>Guan's Website</title><meta name="author" content="Xingquan Guan"><link rel="shortcut icon" href="/img/favicon.png"><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.13.0/css/all.min.css"><meta name="generator" content="Hexo 5.4.2"><link rel="alternate" href="/atom.xml" title="Guan's Website" type="application/atom+xml">
</head><body><header id="page_header"><div class="header_wrap"><div id="blog_name"><a class="blog_title" id="site-name" href="/">Guan's Website</a></div><button class="menus_icon"><div class="navicon"></div></button><ul class="menus_items"><li class="menus_item"><a class="site-page" href="/About"> About</a></li><li class="menus_item"><a class="site-page" href="/Research"> Research</a></li><li class="menus_item"><a class="site-page" href="/Publications"> Publications</a></li><li class="menus_item"><a class="site-page" href="/Teaching"> Teaching</a></li><li class="menus_item"><a class="site-page" href="/Resources"> Resources</a></li><li class="menus_item"><a class="site-page" href="/Blog"> Blog</a></li></ul></div></header><main id="page_main"><div class="side-card sticky"><div class="card-wrap" itemscope itemtype="http://schema.org/Person"><div class="author-avatar"><img class="avatar-img" src="/img/profile.png" onerror="this.onerror=null;this.src='/img/profile.png'" alt="avatar"></div><div class="author-discrip"><h3>Xingquan Guan</h3><p class="author-bio">Data Scientist | Researcher | Hazard Engineering + Artificial Intelligence</p></div><div class="author-links"><button class="btn m-social-links">Links</button><ul class="social-icons"><li><a class="social-icon" href="https://github.com/GUAN-XINGQUAN/" target="_blank"><i class="fab fa-github" aria-hidden="true"></i></a></li><li><a class="social-icon" href="https://www.linkedin.com/in/guanxingquan/" target="_blank"><i class="fab fa-linkedin" aria-hidden="true"></i></a></li><li><a class="social-icon" href="tencent://AddContact/?fromId=50&amp;fromSubId=1&amp;subcmd=all&amp;uin=619460794/" target="_blank"><i class="fab fa-qq" aria-hidden="true"></i></a></li><li><a class="social-icon" href="mailto:guanxingquan@ucla.edu/" target="_blank"><i class="fas fa-envelope" aria-hidden="true"></i></a></li></ul><ul class="social-links"><li><a class="e-social-link" href="https://scholar.google.com/citations?user=KXiOdokAAAAJ&amp;hl=en&amp;authuser=1/ /" target="_blank"><i class="fas fa-graduation-cap" aria-hidden="true"></i><span>Google Scholar</span></a></li><li><a class="e-social-link" href="https://www.researchgate.net/profile/xingquan-guan/ /" target="_blank"><i class="fab fa-researchgate" aria-hidden="true"></i><span>Research Gate</span></a></li></ul></div></div></div><div class="page" itemscope itemtype="http://schema.org/CreativeWork"><h2 class="page-title">Week 1 Linear Regression</h2><article><p>I have been learning the statistics for about one year. Nine months ago, I didn’t believe that I can survive from statistics classes as I don’t know much about statistics. For example, I don’t even know that Gaussian distribution is another name of Normal distribution. I just want write something here to summarize what I have learned during this year.</p>
<p>统计学也陆陆续续的学了一年了。没想到自己除了土木工程还是可以学一些其他的东西。现在利用暑假总结一下自己这一年学过的统计学知识，也算是对自己统计学的辅修有一个交代。</p>
<p>What I am writing here is something I summarized using my own words. These “words” may not be accurate enough as I am an Engineering student, not a student majoring in statistics. I just wrote all this stuff based on my own interest. I hope this note could help someone who doesn’t has much background in statistics but wants to learn some statistics.</p>
<p>这些所写的内容都是基于我自己的理解，里面所用到的一些词语并不是非常准确。毕竟是我只是一个工程学科的学生，不是一个统计学学生。我写下这些东西也只是基于自己的兴趣爱好，同时希望帮助一些像我一样零基础的同学学习统计学。</p>
<p>———————————————– starts from here ———————————————————</p>
<p>Background (问题背景):</p>
<p>Assume that both X and Y are a N by 1 vector. X is called as predictor and Y is called as response.</p>
<p>假定X和Y均是一个N行1列的向量（矩阵）。X被称作自变量，Y被称作因变量。</p>
<p>The relationship between X and Y is presented in the figure below (python code is provided):</p>
<p>X与Y之间的关系见下图所示 (python源代码也附在下面)：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Add this command in order to show the plot in Anaconda</span></span><br><span class="line"><span class="comment"># Otherwise the plot may not show up</span></span><br><span class="line">% matplotlib inline</span><br><span class="line"></span><br><span class="line"><span class="comment"># Import necessary packages</span></span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt  <span class="comment"># Package used for plotting the figure</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np  <span class="comment"># Package for matrix operation</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Define the number of observation</span></span><br><span class="line"><span class="comment"># This number is also the length of X or Y vector</span></span><br><span class="line">number_observation = <span class="number">1000</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Generate X using linear space, i.e., X increase from -5 to 5. The interval is a constant such that there will be 1000 X.</span></span><br><span class="line">X = np.linspace(-<span class="number">5</span>, <span class="number">5</span>, number_observation)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Generate Y</span></span><br><span class="line"><span class="comment"># Assume Y is generated using the equation: Y = 2X + 3 + e</span></span><br><span class="line"><span class="comment"># e refers to the noise, which may be caused by measurement error.</span></span><br><span class="line"><span class="comment"># e is assumed to follow normal distribution with a mean of 0 and a standard deviation of 1</span></span><br><span class="line">e = np.random.normal(<span class="number">0</span>, <span class="number">1</span>, number_observation)</span><br><span class="line">Y = <span class="number">2</span>*X + <span class="number">3</span> + e</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Plot the X and Y</span></span><br><span class="line">plt.figure(figsize=(<span class="number">16</span>,<span class="number">8</span>))</span><br><span class="line">plt.scatter(X, Y, color=<span class="string">&#x27;r&#x27;</span>)</span><br><span class="line">font = &#123;<span class="string">&#x27;family&#x27;</span>:<span class="string">&#x27;Times New Roman&#x27;</span>,<span class="string">&#x27;weight&#x27;</span> : <span class="string">&#x27;normal&#x27;</span>,<span class="string">&#x27;size&#x27;</span>: <span class="number">16</span>,&#125;</span><br><span class="line">plt.xlabel(<span class="string">&#x27;X&#x27;</span>, font)</span><br><span class="line">plt.ylabel(<span class="string">&#x27;Y&#x27;</span>, font)</span><br><span class="line">plt.xticks(fontsize=<span class="number">16</span>, fontname=<span class="string">&#x27;Times New Roman&#x27;</span>)</span><br><span class="line">plt.yticks(fontsize=<span class="number">16</span>, fontname=<span class="string">&#x27;Times New Roman&#x27;</span>)</span><br><span class="line">plt.title(<span class="string">&#x27;The relationship between X and Y&#x27;</span>, font)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>


<p><img src="https://raw.githubusercontent.com/GUAN-XINGQUAN/images/main/img/20210308231915.png"></p>
<p>Now we roughly know the relationship between X and Y. It seems that Y is linearly dependent on X. Therefore we try to use a linear function to fit these data points.</p>
<p>现在我们大致了解X与Y之间的关系。通过图可以看出，Y似乎和X呈现出一种线性关系。因此我们需要用线性函数来拟合这些离散的数据点。</p>
<p>The linear function is presented below:</p>
<p>线性函数的形式如下所示：</p>
<p>$h(x_i)=\beta_1 X_i + \beta_0$</p>
<p>Where $h(x_i)$ is the predicted response when $x_i$ is given.</p>
<p>函数中，$h(x_i)$是当给定$X_i$值时，预测的Y值。</p>
<p>Now, we only need to determine the values for $\beta_1$ and $\beta_0$ such that $h(x_i)$ is close to $Y_i$ as much as possible.</p>
<p>现在，我们需要确定$\beta_1$和$\beta_0$的值，使得我们预测出的每一个 $h(x_i)$都尽可能的接近真实值$y_i$.</p>
<p>Therefore, we need to create a indicator to quantify how close the predicted response is close to actual response. The mean squared error loss function is used here as the indicator:</p>
<p>因此我们需要建立一个量化指标来衡量到底预测值$h(x_i)$与真实值$y_i$有多接近。我们将使用平方和损失函数（目标函数）作为这个量化指标：</p>
<p>$L(\beta_0, \beta_1) = \frac{1}{N}\sum_{i=1}^N(\beta_0+\beta_1 x_i - y_i)^2$</p>
<p>Based on this loss function, we try to find optimized values for $\beta_0$ and $\beta_1$ such that the loss is minimized.</p>
<p>借助于这个损失函数，我们试图寻找$\beta_0$和$\beta_1$的最优解，在这个最优解下，损失函数的值最小。</p>
<p>If this problem is solved manually, we can just take the derivative of the loss function with respect to $\beta_1$ or $\beta_0$. In this case, we can obtain the optimized values directly.</p>
<p>如果我们试图用手算来解决这个问题，我们可以直接将损失函数对$\beta_0$和$\beta_1$求导数，即可获得$\beta_0$和$\beta_1$的解。</p>
<p>Here, we would like to solve this problem using computer programming. Therefore, optimization algorith gradient descent is involved.</p>
<p>这里，我们想写计算机代码来解决这个问题，因此我们要用到叫做“梯度下降”的优化算法。</p>
<p>Graident descent will be introduced later in this session.</p>
<p>梯度下降的算法将会在稍后进行介绍。</p>
<p>The plot for the loss function is presented below (python code is also provided):</p>
<p>平方和损失函数如下图所示（python源代码一并提供）：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Import the package for three-dimensional plot</span></span><br><span class="line"><span class="keyword">from</span> mpl_toolkits.mplot3d <span class="keyword">import</span> Axes3D</span><br><span class="line"></span><br><span class="line"><span class="comment"># Generate values for beta_0 and beta_1</span></span><br><span class="line">number_beta = <span class="number">100</span></span><br><span class="line">beta_0 = np.linspace(-<span class="number">5</span>, <span class="number">10</span>, number_beta)</span><br><span class="line">beta_1 = np.linspace(-<span class="number">1</span>, <span class="number">5</span>, number_beta)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Create a grid to evaluate loss function values on the grid</span></span><br><span class="line"><span class="comment"># You can either use command in line #11 or comand in line #18, 19, 23, 24 to create grid</span></span><br><span class="line"><span class="comment"># grid_beta_0, grid_beta_1 = np.meshgrid(beta_0, beta_1)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Calculate the loss function values under different beta_0 and beta_1</span></span><br><span class="line"><span class="comment"># Initialize a matrix to store loss function values</span></span><br><span class="line">loss = np.zeros([number_beta, number_beta])</span><br><span class="line"></span><br><span class="line"><span class="comment"># Initialize a matrix to store grid values</span></span><br><span class="line">grid_beta_0 = np.zeros([number_beta, number_beta])</span><br><span class="line">grid_beta_1 = np.zeros([number_beta, number_beta])</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(number_beta):</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(number_beta):</span><br><span class="line">        grid_beta_0[i,j] = beta_0[i]</span><br><span class="line">        grid_beta_1[i,j] = beta_1[j]</span><br><span class="line">        predict_response = beta_0[i] + beta_1[j] * X</span><br><span class="line">        loss[i,j] = np.dot((predict_response - Y).T, (predict_response-Y))/number_observation</span><br><span class="line"></span><br><span class="line"><span class="comment"># Plot the loss function values over the beta_0 and beta_1 values</span></span><br><span class="line">fig = plt.figure(figsize=(<span class="number">16</span>,<span class="number">8</span>))</span><br><span class="line">ax = Axes3D(fig)</span><br><span class="line">ax.set_xlabel(<span class="string">r&quot;$\beta_0$&quot;</span>, font)</span><br><span class="line">ax.set_ylabel(<span class="string">r&quot;$\beta_1$&quot;</span>, font)</span><br><span class="line">ax.set_zlabel(<span class="string">&quot;Loss function value&quot;</span>, font)</span><br><span class="line">plt.xticks(fontsize=<span class="number">16</span>, fontname=<span class="string">&#x27;Times New Roman&#x27;</span>)</span><br><span class="line">plt.yticks(fontsize=<span class="number">16</span>, fontname=<span class="string">&#x27;Times New Roman&#x27;</span>)</span><br><span class="line">ax.plot_surface(grid_beta_0, grid_beta_1, loss, cmap=<span class="string">&#x27;rainbow&#x27;</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>


<p><img src="https://raw.githubusercontent.com/GUAN-XINGQUAN/images/main/img/20210308231952.png"></p>
<p>The figure above shows that a global minimum exists for this loss function. This global minimum is the case that we want to achieve because the loss is smallest. In other words, the predicted response is closest to the actual response.</p>
<p>上图展示了我们损失函数有一个全局最小值，而这个最小值正是我们试图寻找的。该最小值表征了我们预测的Y值与真实Y值是最接近的。</p>
<p>As mentioned above, we are using gradient descent to find the global minimum. For those who are not familiar with gradient descent, please refer to the following link for more information:</p>
<p><a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Gradient_descent">https://en.wikipedia.org/wiki/Gradient_descent</a></p>
<p>如上文提到的，我们将使用梯度下降方法来寻找全局最小值。对于还不是很熟悉梯度下降概念的同学，可以点击下面链接来了解梯度下降方法：</p>
<p><a target="_blank" rel="noopener" href="https://baike.baidu.com/item/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D/4864937?fr=aladdin">https://baike.baidu.com/item/梯度下降/4864937?fr=aladdin</a></p>
<p>For gradient descent algorithm, the first step is to give arbitary values for $\beta_0$ and $\beta_1$.</p>
<p>梯度下降方法中，我们先随机赋予$\beta_0$和$\beta_1$两个数值。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># These two values are given arbitrarily</span></span><br><span class="line"><span class="comment"># You can give any values as you like</span></span><br><span class="line">beta_0 = <span class="number">1</span></span><br><span class="line">beta_1 = <span class="number">0.5</span></span><br><span class="line">predict_response = beta_0 + beta_1 * X</span><br><span class="line"></span><br><span class="line"><span class="comment"># Visualize the fitted line</span></span><br><span class="line">plt.figure(figsize=(<span class="number">16</span>,<span class="number">8</span>))</span><br><span class="line">plt.scatter(X, Y, color=<span class="string">&#x27;r&#x27;</span>, label=<span class="string">&#x27;original data points&#x27;</span>)</span><br><span class="line">plt.plot(X, predict_response, color=<span class="string">&#x27;g&#x27;</span>, linewidth=<span class="number">2.0</span>, label=<span class="string">&#x27;fitted line&#x27;</span>)</span><br><span class="line">font = &#123;<span class="string">&#x27;family&#x27;</span>:<span class="string">&#x27;Times New Roman&#x27;</span>,<span class="string">&#x27;weight&#x27;</span> : <span class="string">&#x27;normal&#x27;</span>,<span class="string">&#x27;size&#x27;</span>: <span class="number">16</span>,&#125;</span><br><span class="line">plt.xlabel(<span class="string">&#x27;X&#x27;</span>, font)</span><br><span class="line">plt.ylabel(<span class="string">&#x27;Y&#x27;</span>, font)</span><br><span class="line">plt.xticks(fontsize=<span class="number">16</span>, fontname=<span class="string">&#x27;Times New Roman&#x27;</span>)</span><br><span class="line">plt.yticks(fontsize=<span class="number">16</span>, fontname=<span class="string">&#x27;Times New Roman&#x27;</span>)</span><br><span class="line">plt.title(<span class="string">&#x27;Original data points and fitted line&#x27;</span>, font)</span><br><span class="line">plt.legend(fontsize=<span class="number">16</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>


<p><img src="https://raw.githubusercontent.com/GUAN-XINGQUAN/images/main/img/20210308232044.png"></p>
<p>As shown in the figure above, the initial fitted line does not match the data points. This is not surprised as the values of $\beta_0$ and $\beta_1$ are given arbritarily.</p>
<p>上图展示了拟合的直线与离散的数据点吻合并不是很好。这并不出乎意料，因为我们只是随意的给$\beta_0$和$\beta_1$赋值。</p>
<p>Don’t worry, we will use gradient descent to “train” $\beta_0$ and $\beta_1$. Eventually, a good fitted line will be obtained.</p>
<p>我们会用梯度下降来“训练”$\beta_0$和$\beta_1$，使得最终获得的较好的拟合效果。</p>
<p>We need to find the gradient of the loss function, i.e., the derivative with respect to $\beta_0$ and $\beta_1$:</p>
<p>首先，我们需要寻找函数的“梯度”，即关于$\beta_0$和$\beta_1$的导数：</p>
<p>$\frac{\partial L(\beta_0,\beta_1)}{\partial \beta_0} = \frac{2}{N}\sum_{i=1}^{N}(\beta_0 + \beta_1 x_i - y_i)$</p>
<p>$\frac{\partial L(\beta_0,\beta_1)}{\partial \beta_1} = \frac{2}{N}\sum_{i=1}^{N}(\beta_0 + \beta_1 x_i - y_i)x_i$</p>
<p>Then we just need to use the negative gradient direction to update $\beta_0$ and $beta_1$</p>
<p>然后，我们沿着梯度的负方向来迭代更新$\beta_0$和$\beta_1$两个参数。</p>
<p>$\beta_0^{n+1} = \beta_0^n - s\frac{\partial L(\beta_0^n, \beta_1^n)}{\partial \beta_0^n}$</p>
<p>$\beta_1^{n+1} = \beta_1^n - s\frac{\partial L(\beta_0^n, \beta_1^n)}{\partial \beta_1^n}$</p>
<p>where s is the step size. Step size cannot be neither too large nor too small. If it is too large, then we may miss the global minimum value. If it is too small, this update algorithm would be time-consuming.</p>
<p>表达式中的s是代表步长。步长不能太长，也不能太短。步长如果太长，会使得我们错过了全局最小值。如果步长太短，会使得算法花很长的时间来收敛。</p>
<p>Then the question is how to determine the value for step size. Honestly, I don’t know the answer. Evertime I did this. I always given an arbitrary value for the step size. If it doesn’t work well, then I just change the value for step size, and try again. It’s like a trial and error process.</p>
<p>那么如何确定步长的大小。这是一个“玄学”。至少我本人不是很了解如何确定。我通常的做法是给定一个步长，跑整个程序。如果结果不好，我再修改步长，再运行，直到结果比较满意为止。</p>
<p>The following section gives the code on how we perform gradience descent algorithm. The final results are also shown below.</p>
<p>接下来的代码展示了我们如何使用梯度下降方法。最终结果也一并画图呈现。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Assume we are going to iterate 5000 times</span></span><br><span class="line">max_iteration = <span class="number">5000</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Assume step size is 0.001</span></span><br><span class="line">step_size = <span class="number">0.001</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Initialize beta_0, beta_1, and loss to track the update algorithm</span></span><br><span class="line">beta_0_update = np.zeros([max_iteration+<span class="number">1</span>, <span class="number">1</span>])</span><br><span class="line">beta_1_update = np.zeros([max_iteration+<span class="number">1</span>, <span class="number">1</span>])</span><br><span class="line">loss_update = np.zeros([max_iteration+<span class="number">1</span>, <span class="number">1</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># Initial value for beta_0 and beta_1</span></span><br><span class="line">beta_0_update[<span class="number">0</span>,<span class="number">0</span>] = beta_0</span><br><span class="line">beta_1_update[<span class="number">0</span>,<span class="number">0</span>] = beta_1</span><br><span class="line"></span><br><span class="line"><span class="comment"># Iterate many times in order to get the best values for beta_0 and beta_1</span></span><br><span class="line"><span class="keyword">for</span> <span class="built_in">iter</span> <span class="keyword">in</span> <span class="built_in">range</span>(max_iteration):</span><br><span class="line">    predict_response = beta_0_update[<span class="built_in">iter</span>,<span class="number">0</span>] + beta_1_update[<span class="built_in">iter</span>,<span class="number">0</span>] * X</span><br><span class="line">    difference = predict_response - Y</span><br><span class="line">    loss_update[<span class="built_in">iter</span>,<span class="number">0</span>] = np.dot(difference.T, difference)/number_observation</span><br><span class="line">    gradient_beta_0 = <span class="number">2</span> * np.<span class="built_in">sum</span>(difference)/number_observation</span><br><span class="line">    gradient_beta_1 = <span class="number">2</span> * np.dot(difference, X)/number_observation</span><br><span class="line">    beta_0_update[<span class="built_in">iter</span>+<span class="number">1</span>] = beta_0_update[<span class="built_in">iter</span>] - step_size * gradient_beta_0</span><br><span class="line">    beta_1_update[<span class="built_in">iter</span>+<span class="number">1</span>] = beta_1_update[<span class="built_in">iter</span>] - step_size * gradient_beta_1</span><br><span class="line"></span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">Y_predict = beta_0_update[-<span class="number">1</span>,<span class="number">0</span>]+ beta_1_update[-<span class="number">1</span>,<span class="number">0</span>] * X</span><br><span class="line"></span><br><span class="line">plt.figure(figsize=(<span class="number">16</span>,<span class="number">8</span>))</span><br><span class="line">plt.scatter(X, Y, color=<span class="string">&#x27;r&#x27;</span>, label=<span class="string">&#x27;original data points&#x27;</span>)</span><br><span class="line">plt.plot(X, Y_predict, color=<span class="string">&#x27;g&#x27;</span>, linewidth=<span class="number">2.0</span>, label=<span class="string">&#x27;fitted line&#x27;</span>)</span><br><span class="line">font = &#123;<span class="string">&#x27;family&#x27;</span>:<span class="string">&#x27;Times New Roman&#x27;</span>,<span class="string">&#x27;weight&#x27;</span> : <span class="string">&#x27;normal&#x27;</span>,<span class="string">&#x27;size&#x27;</span>: <span class="number">16</span>,&#125;</span><br><span class="line">plt.xlabel(<span class="string">&#x27;X&#x27;</span>, font)</span><br><span class="line">plt.ylabel(<span class="string">&#x27;Y&#x27;</span>, font)</span><br><span class="line">plt.xticks(fontsize=<span class="number">16</span>, fontname=<span class="string">&#x27;Times New Roman&#x27;</span>)</span><br><span class="line">plt.yticks(fontsize=<span class="number">16</span>, fontname=<span class="string">&#x27;Times New Roman&#x27;</span>)</span><br><span class="line">plt.title(<span class="string">&#x27;Original data points and final fitted line&#x27;</span>, font)</span><br><span class="line">plt.legend(fontsize=<span class="number">16</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>


<p><img src="https://raw.githubusercontent.com/GUAN-XINGQUAN/images/main/img/20210308232110.png"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">plt.figure(figsize=(<span class="number">16</span>,<span class="number">8</span>))</span><br><span class="line">plt.plot(<span class="built_in">range</span>(max_iteration), loss_update[<span class="number">0</span>:max_iteration,<span class="number">0</span>], color=<span class="string">&#x27;g&#x27;</span>, linewidth=<span class="number">2.0</span>, label=<span class="string">&#x27;fitted line&#x27;</span>)</span><br><span class="line">font = &#123;<span class="string">&#x27;family&#x27;</span>:<span class="string">&#x27;Times New Roman&#x27;</span>,<span class="string">&#x27;weight&#x27;</span> : <span class="string">&#x27;normal&#x27;</span>,<span class="string">&#x27;size&#x27;</span>: <span class="number">16</span>,&#125;</span><br><span class="line">plt.xlabel(<span class="string">&#x27;iteration times&#x27;</span>, font)</span><br><span class="line">plt.ylabel(<span class="string">&#x27;loss function values&#x27;</span>, font)</span><br><span class="line">plt.xticks(fontsize=<span class="number">16</span>, fontname=<span class="string">&#x27;Times New Roman&#x27;</span>)</span><br><span class="line">plt.yticks(fontsize=<span class="number">16</span>, fontname=<span class="string">&#x27;Times New Roman&#x27;</span>)</span><br><span class="line">plt.title(<span class="string">&#x27;Loss function value vs. iteration times&#x27;</span>, font)</span><br><span class="line">plt.legend(fontsize=<span class="number">16</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>


<p><img src="https://raw.githubusercontent.com/GUAN-XINGQUAN/images/main/img/20210308232133.png"></p>
<p>As observed from the plot of loss function value vs. iteration times, the loss already reaches convergence. This means that even we increase the iteration times, the loss would not further decrease. The final fitted line and data points are shown in the figure above.</p>
<p>从损失函数与迭代次数的关系图象可以看出，损失函数早已收敛。这意味着，即便我们增加迭代次数，损失函数的值不会进一步减小。最终拟合曲线与离散点在上文的图也已展示。</p>
<p>OK. We are done. Feel free to leave any comments.</p>
<p>到此结束。</p>
</article></div></main><div class="nav-wrap"><div class="nav"><button class="site-nav"><div class="navicon"></div></button><ul class="nav_items"><li class="nav_item"><a class="nav-page" href="/About"> About</a></li><li class="nav_item"><a class="nav-page" href="/Research"> Research</a></li><li class="nav_item"><a class="nav-page" href="/Publications"> Publications</a></li><li class="nav_item"><a class="nav-page" href="/Teaching"> Teaching</a></li><li class="nav_item"><a class="nav-page" href="/Resources"> Resources</a></li><li class="nav_item"><a class="nav-page" href="/Blog"> Blog</a></li></ul></div><div class="cd-top"><i class="fa fa-arrow-up" aria-hidden="true"></i></div></div><footer id="page_footer"><div class="footer_wrap"><div class="copyright">&copy;2020 - 2022 by Xingquan Guan</div><div class="theme-info">Powered by <a target="_blank" href="https://hexo.io" rel="nofollow noopener">Hexo</a> & <a target="_blank" href="https://github.com/PhosphorW/hexo-theme-academia" rel="nofollow noopener">Academia Theme</a></div></div></footer><script src="https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/jquery-pjax@latest/jquery.pjax.min.js"></script><script src="/js/main.js"></script></body></html>